\documentclass[12pt,a4paper,oneside]{scrartcl}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{tcolorbox}
\usepackage{microtype}
\usepackage{geometry}
\geometry{left=3cm,right=2.5cm,top=3cm,bottom=3cm}
\usepackage{setspace}
\onehalfspacing

\usepackage{amsmath,amssymb,amsthm,mathtools}
\usepackage{bm}
\usepackage{dsfont}
\usepackage{physics}
\usepackage{nicefrac}
\usepackage{enumitem}
\setlist[itemize]{topsep=0pt, itemsep=2pt}

\usepackage{tikz}
\usetikzlibrary{arrows.meta, positioning}
\usepackage{pgfplots}
\pgfplotsset{compat=1.17}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{etoolbox}
\usepackage{xcolor}
\usepackage{float}
\usepackage{wrapfig}

\KOMAoptions{parskip=half}

\definecolor{indigo}{RGB}{75,0,130}

% Theorem
\newtheorem{lemma}{Lemma}[section]
\newtheorem{definition}{Definition}[section]
\newtheorem{theorem}{Theorem}[section]

\newenvironment{lemmabox}[1][]{
	\refstepcounter{lemma}
	\begin{tcolorbox}[colback=teal!5!white,
		colframe=teal!60!black,
		fonttitle=\bfseries,
		boxrule=0.8pt,
		arc=3pt,
		left=8pt,right=8pt,top=6pt,bottom=6pt,
		title={Lemma \thelemma\if\relax\detokenize{#1}\relax\else\ (#1)\fi}]
		}{
	\end{tcolorbox}
}

\newenvironment{definitionbox}[1][]{
\refstepcounter{definition}
\begin{tcolorbox}[colback=blue!5!white,
	colframe=blue!60!black,
	fonttitle=\bfseries,
	boxrule=0.8pt,
	arc=3pt,
	left=8pt,right=8pt,top=6pt,bottom=6pt,
	title={Definition \thedefinition\if\relax\detokenize{#1}\relax\else\ (#1)\fi}]
	}{
\end{tcolorbox}
}

\newenvironment{theorembox}[1][]{
\refstepcounter{theorem}
\begin{tcolorbox}[colback=indigo!5!white,
	colframe=indigo!70!black,
	fonttitle=\bfseries,
	boxrule=0.8pt,
	arc=3pt,
	left=8pt,right=8pt,top=6pt,bottom=6pt,
	title={Theorem \thetheorem\if\relax\detokenize{#1}\relax\else\ (#1)\fi}]
}{
\end{tcolorbox}
}

% Kopf- und Fußzeile
\usepackage{scrlayer-scrpage}
\clearpairofpagestyles
\ihead{Optimal Control}
\ohead{WS 2025/26}
\cfoot{\pagemark}
\pagestyle{scrheadings}
\setcounter{section}{-1}

\begin{document}
	
	% Titelseite
	\begin{titlepage}
		\centering
		\vspace*{3cm}
		{\Huge\bfseries Optimal Control}\par
		\vspace{1.5cm}
		{\Large Wintersemester 2025/26}\par
		\vspace{0.5cm}
		{\Large Dozent: Prof. Dr. Andrea Iannelli}\par
		\vfill
		{\today}
	\end{titlepage}
	
	\pagenumbering{roman}
	\tableofcontents
	\clearpage
	\pagenumbering{arabic}
	
	\section{Introduction}
	\[
	\dot{x} = f(t,x,u), \quad x(t_0)=x_0, \quad t \in [t_0,t_f]
	\]
	\[
	f : [t_0,t_f]\times\mathbb{R}^{n_x}\times\mathbb{R}^{n_u} \to \mathbb{R}^{n_x}
	\]
	\[
	x = \text{state}, \quad u = \text{input}
	\]
	
	Initial Value Problem (IVP)
	
	Given $x_0, u(\cdot)$ we can compute $x(\cdot)$ \\
	\hspace*{23.5mm}$\rotatebox[origin=c]{90}{$\Rsh$}$ functions of time $\rotatebox[origin=c]{270}{$\Lsh$}$
	
	When is this possible? It depends on $f$.
	
	
	\begin{lemmabox}[Sufficient conditions]
		Existence \& Uniqueness of solutions of ODEs.\\
		Assume that
		\begin{itemize}[]
			\item $f$ is piecewise continuous in $t$ and $u$
			\item $f$ is globally Lipschitz in $x$
			\[
			\exists\, k(t,u)\, \text{ s.t. } \|f(t,x_1,u)-f(t,x_2,u)\|\le k(t,u)\|x_1-x_2\|,\ \forall x_1,x_2 \in \mathbb{R}^{n_x}
			\]
		\end{itemize}
		Then $x(\cdot)$ exists for all $t$ and is unique.
	\end{lemmabox}
	
	\paragraph{Remarks}
	\begin{itemize}[noitemsep]
		\item Lipschitz continuous $\Rightarrow$ continuous, but not the converse
		\item $\sqrt{x}$ is continuous but not Lipschitz, $\dot x = \sqrt{x}$ does not have a unique solution
		\item Continously differentiable $(\mathcal{C}^1)$ $\Rightarrow$ locally Lipschitz continous $\forall x_1,x_2 \in \mathcal{X} \subset \mathbb{R}^{n_x}$
		\item Locally Lipschitz continuous x guarantees existence \& uniqueness for small enough times
	\end{itemize}
	
	In this course we will assume $f \in\mathcal{C}^1$ and implicitely assume that $t_f$ is chosen such that $x(\cdot)$ exists in $[t_0,t_f]$. \\\\
	We do not need to worry about existence \& uniqueness!
	
	\paragraph{Goal in Optimal Control:}
	Design $u$ such that
	\begin{enumerate}
		\item $u(t) \in \underset{\uparrow}{\mathcal{U}(t)}$, $x(t) \in \underset{\uparrow}{\mathcal{X}(t)} \quad \forall t \in [t_0,t_f], \quad \mathcal{X}\subseteq\mathbb{R}^{n_x}, \ \mathcal{U}\subseteq\mathbb{R}^{n_u}$\\
			sets defining constraints on $u \& x$ \\
			\hspace*{10mm}$\Rightarrow$ Admissible input/state trajectories
		\item
			The system behaves optimally according to
			\[
			\underset{\uparrow}{J}(u) = \int_{t_0}^{t_f} \underset{\uparrow}{l}(t,x(t),u(t))\,dt + \underset{\uparrow}{\varphi}(t_f,x(t_f))
			\]
			\hspace*{18mm}Cost function\hspace*{10mm}running cost\hspace*{10mm}terminal cost\\
			\hspace*{10mm}$\Rightarrow$ optimal behaviour
	\end{enumerate}
	
	Formally, we can state the goal as follows: \\
	Find an admissible input $u^\star$ which causes the dynamics to follow an admissible trajectory $x^\star$ which minimizes $J$, that is
	\[
	\int_{t_0}^{t_f} l(t,x^\star(t),u^\star(t))\,dt + \varphi(t_f,x^\star(t_f)) \leq \int_{t_0}^{t_f} l(t,x(t),u(t))\,dt + \varphi(t_f,x(t_f))
	\]
	\hspace*{100mm}$\forall \text{ admissible } x,u$
	
	\paragraph{Examples of cost functions}
	\begin{enumerate}[label=\arabic*)]
		\item Minimum-time problem\\
		Goal: transfer the system from $x_0$ to a set $\mathcal{S}$ in the minimum time
		\[
		J = t_f-t_0 = \int_{t_0}^{t_f}dt \qquad (l=1, \varphi=0)
		\]
		\[
		x(t_f) \in \mathcal{S}
		\]
		Note: $t_f$ is also a decision variable! The unknowns are $(u,t_f)$.
		
		\item Minimum control-effort problem
		\[
		J = \int_{t_0}^{t_f} \|u(t)\|^2 \, dt
		\]
		\[
		x(t_f) \in \mathcal{S}
		\]
		
		\item Tracking problem
		\[
		J = \int_{t_0}^{t_f} (x(t)-r(t))^T Q (x(t)-r(t))dt
		\]
		$Q > 0$ (positive definit matrix: symmetric \& all eigenvalues positive) \\
		$r(t)$ given signal
	\end{enumerate}
	
	\section{Nonlinear Programming}
	
	Nonlinear Programs (NLP) are general \underline{finite-dimensional} optimization problems:
	\[
	\underset{x}{\min} f(x)
	\]
	\[
	\text{s.t. } g(x)\leq0, \quad h(x)=0
	\]
	$f: \mathbb{R}^n \to \mathbb{R}$, objective function\\
	$g: \mathbb{R}^n \to \mathbb{R}^{n_g}$, inequality constraints\\
	$h: \mathbb{R}^n \to \mathbb{R}^{n_h}$, equality constraints\\
	Feasible set:
	\[
	D = \{x\in\mathbb{R}^n \mid g(x)\leq0,\ h(x)=0\}
	\]
	$\bar{x}\in D$ feasible point
	
	\begin{definitionbox}[Global, local Minimizers]
			$x^\star\in \mathcal{D}$ \underline{Global Minimizer} of the NLP if
			\[
			f(x^\star)\le f(x)\quad \forall x\in \mathcal{D}
			\]
			$f(x^\star)$ is the \underline{Global Minimum} (or Minimum)\\
			Nomenclature: $x^\star$ is also called (optimal) solution, $F(x^\star)$ is optimal value\\
			$x^\star$ is a strict global minimizer if $f(x^\star)<f(x)\quad \forall x \in\mathcal{D}$\\
			$x^\star\in \mathcal{D}$ \underline{Local Minimizer} if
			\[
			\exists \varepsilon>0,\text{ s.t.}\ f(x^\star)\le f(x)\quad \forall x\in B_\varepsilon(x^\star)\cap \mathcal{D}
			\]
			\[
			B_\varepsilon(x) \coloneqq \{ y \mid \|x-y\|\le\varepsilon\} \qquad \|\cdot\|:\mathbb{R}^n\to\mathbb{R}_{\geq 0}\text{ any norm in }\mathbb{R}^n
			\]
			Strict local Minimizer if inequality holds strictly\\
			Global min $\begin{tikzpicture}[baseline={(0,-0.25ex)}]
				% oberer Pfeil: nach rechts
				\draw[->] (0,0.2) -- (0.5,0.2);
				% unterer Pfeil: nach links
				\draw[<-] (0,0.0) -- (0.5,0.0);
				% Durchstreichung des unteren Pfeils
				\draw[line width=0.4pt] (0.1,-0.1) -- (0.4, 0.1);
			\end{tikzpicture}$ local min
	\end{definitionbox}
	
	Solving an NLP boils down to finding global or local minimizers. \\
	Does a solution always exist? No.
	
	
	
	
	\begin{definitionbox}[infimum]
		Given $\mathcal{S} \subseteq \mathbb{R}$, $\inf(\mathcal{S})$ is the greatest lower bound of $\mathcal{S}$:
		\begin{itemize}
			\item $z \geq \inf(\mathcal{S}), \quad \forall z \in \mathcal{S}\quad$ (lower bound)
			\item $\forall \bar{\alpha} > \inf(\mathcal{S}) \quad \exists z \in \mathcal{S}$ s. t. $ \bar{\alpha} > z\quad$ (greatest bound)
		\end{itemize}
	\end{definitionbox}
	
	\paragraph{Example}
		$\mathcal{S} = [-1,1], \, -50 = \inf(\mathcal{S})?\quad\to\quad$ No, $\inf(\mathcal{S}) = -1$	
	\begin{itemize}
		\item Analogous: $\sup(\mathcal{S})$ is smallest upper bound.  
		\item $\inf$ and $\sup$ always exist if $\mathcal{S} \neq \emptyset$
		\item $\inf(\mathcal{S})$ does not have to be an element of $\mathcal{S}$
		\item If $\mathcal{S}$ unbounded from below $\to$ $\inf(\mathcal{S})=-\infty$
		\item $\inf([a,b]) = \inf((a,b]) = a$
	\end{itemize}
	
	Connections with NLP?
	\[
	f: \mathcal{D} \to \mathbb{R}
	\]
	\[
	\inf(\underset{\mathcal{S}}{\underbrace{f(x)\,|\,x\in\mathcal{D}}})\coloneqq\bar{f}=\underset{x\in\mathcal{D}}{\inf}f(x) \quad\text{(similar to NLP)}
	\]
	
	Whenever NLP has solution, then NLP ist equivalent to this, but $\nexists x^\star \in \mathcal{D}$ s. t. $f(x^\star) = \bar{f}\quad\to\quad$ infimum exists, but not minimum
	
	\paragraph{Examples}
		$f(x) = e^{-x}, \quad \mathcal{D} = [0,\infty), \quad \inf(\mathcal{S}) = 0$ \\
		$f(x) = x, \quad \mathcal{D} =\mathbb{R}, \quad \inf(\mathcal{S}) = - \infty$, min doesn't exist! 
	\begin{center}
		\begin{tikzpicture}
			\begin{axis}[
				axis lines = left,
				grid = both,
				xlabel = $x$,
				ylabel = $f(x)$,
				xmin = 0, xmax = 5,
				ymin = 0, ymax = 1.2,
				width = 10cm,
				height = 5cm,
				every axis plot/.style={very thick, green!60!black},
				]
				
				\addplot[domain=0:5, samples=200]{exp(-x)} node[pos=0.15, above right] {$f$};
				\node at (axis cs:0,1) [anchor=east, green!50!black] {$f$};
				
			\end{axis}
		\end{tikzpicture}
	\end{center}
	When does the infimum coincide with the minimum?
	
	\begin{theorembox}[Extreme value problem) (Weierstrass Theorem]
		$f: \mathcal{D} \to \mathbb{R},\,\mathcal{D} \subseteq \mathbb{R}^n$\\
		If:
		\begin{itemize}
			\item $f \in \mathcal{C}$ on $\mathcal{D}$
			\item $\mathcal{D}$ is compact
			\item $\mathcal{D} \neq \emptyset$
		\end{itemize}
		Then $f$ attains a minimum on $\mathcal{D}$.
	\end{theorembox}
	
	\begin{definitionbox}[Continuous function]
		$f: \mathcal{D} \to \mathbb{R}$ is continuous at $x\in\mathcal{D}$ if
		\[
			\forall \varepsilon > 0 \exists \delta > 0 \text{ s. t. } \norm{x-x'} < \delta \quad \Rightarrow \quad \norm{f(x)-f(x')} < \varepsilon
		\]
		If $f$ is continuous $\forall x \in \mathcal{D}$ then $f$ is continuous on $\mathcal{D}$ $\to$ $f \in \mathcal{C}$
	\end{definitionbox}
	
	Implication for NLP: If $f$ is $\mathcal{C}$ on $\mathcal{D}$ and $\mathcal{D}$ is compact and non-empty then [NLP] has a solution!
	
	\begin{itemize}
		\item $\mathcal{D} \subseteq \mathbb{R}^n$: in finite-dimensional spaces: compact = closed and bounded\\
		Not compact:
		\begin{itemize}
			\item $(a,b]\quad$ (not closed)
			\item $(-\infty,b]\quad$ (unbounded)
		\end{itemize}
		Compact set:
		\begin{itemize}
			\item $[a,b]\quad-\infty<a<b<\infty$
		\end{itemize}
	
	\paragraph{Warning:}
	$\mathcal{D}$ infinite dimensional (e.\,g. function space) then\\
	\hspace*{10mm}compact $\begin{tikzpicture}[baseline={(0,-0.25ex)}]
		% oberer Pfeil: nach rechts
		\draw[->] (0,0.2) -- (0.5,0.2);
		% unterer Pfeil: nach links
		\draw[<-] (0,0.0) -- (0.5,0.0);
		% Durchstreichung des unteren Pfeils
		\draw[line width=0.4pt] (0.1,-0.1) -- (0.4, 0.1);
	\end{tikzpicture}$ bounded and closed
	\begin{center}
		\begin{tikzpicture}
			\begin{axis}[
				axis lines = middle,
				xlabel={$x$},
				ylabel={$f(x)$},
				xmin=-0.7, xmax=0.7,
				ymin=-0.0, ymax=0.45,
				width=6cm,
				height=4cm,
				grid=both,
				every axis plot/.style={very thick, green!60!black},
				]
				
				% Funktion
				\addplot[domain=-0.7:0.7, samples=200]{x^2};
				
				% Minimum markieren
				\addplot[only marks, mark=* , mark size=2.5pt, red] coordinates {(0,0)};
				\node[red, above right] at (axis cs:0,0) {$x^\star$};
				
			\end{axis}
		\end{tikzpicture}
	\end{center}
	Theorem 1.1 is restrictive e.\,g. $f(x) = x^2, \mathcal{D} = (-\infty,\infty)$ has unique minimum
	\item Notation convention: Technically it is ``wrong'' to write
	\[
		\underset{x\in\mathcal{D}}{\min}\;f(x)
	\]
	more compact is:
	\[
		\text{minimize}\;\underset{x\in\mathcal{D}}{f(x)} \quad \text{or} \quad \underset{x\in\mathcal{D}}{\inf}\;f(x)
	\]
	\end{itemize}
	
	Goal of the Chapter: characterize necessary and sufficient conditions for $x^\star$ to be global minimizer of NLP.
	
	\subsubsection*{Convexity}
	
	\begin{definitionbox}[Convex sets \& functions]
		\begin{itemize}
			\item A set $C \subseteq \mathbb{R}^n$ is \underline{convex} (cvx) if $\forall x, y \in C$
			\[
			\{ z \mid z = \lambda x + (1-\lambda)\lambda, \, \lambda \in [0,1]\}\subseteq C
			\]
			\begin{center}
				\begin{tikzpicture}[>=Latex]
				
				% Ellipse
				\draw[blue, line width=1pt] (0,0) ellipse [x radius=3.2cm, y radius=1.6cm];
				
				% Punkte
				\coordinate (X) at (-1.25,0.20);
				\coordinate (Y) at ( 1.10,-0.10);
				
				% Verbindungssegment
				\draw[gray!70, line width=0.8pt] (X) -- (Y);
				
				% Markierungen + Labels
				\draw[red!75!black, fill=red!75!black] (X) circle (2.1pt)
				node[above left=1pt, text=red!75!black] {$x$};
				\draw[red!75!black, fill=red!75!black] (Y) circle (2.1pt)
				node[below right=1pt, text=red!75!black] {$y$};
				% Buchstabe C in der Ellipse
				\node[blue] at (-0.25,-0.5) {$C$};
				\end{tikzpicture}
			\end{center}
			
			\item Given a cvx set $C$, a function $f: C \to \mathbb{R}$ is cvx if
			\[
			\textcolor{green!50!black}{f(\lambda x + (1 - \lambda)y)} \leq \textcolor{ purple!60!white}{\lambda f(x) + (1 - \lambda) f(y)}, \quad\forall x,y \in C, \quad\lambda \in (0,1)
			\]
			\begin{center}
				\begin{tikzpicture}[line cap=round, line join=round]
					
					% Parabel: y = a*x^2 + c  (nach oben geöffnet)
					\pgfmathsetmacro\a{0.35}
					\pgfmathsetmacro\c{-1.8}
					
					% Punkte auf der Parabel (x-Koordinaten, xA < xB)
					\pgfmathsetmacro\xA{-2.0}
					\pgfmathsetmacro\xB{ 2.4}
					\pgfmathsetmacro\yA{\a*\xA*\xA + \c}
					\pgfmathsetmacro\yB{\a*\xB*\xB + \c}
					
					% Gerade durch die beiden Punkte
					\pgfmathsetmacro\m{(\yB-\yA)/(\xB-\xA)}
					\pgfmathsetmacro\b{\yA - \m*\xA}
					
					% --- Parabel zuerst komplett SCHWARZ ---
					\draw[very thick, black, domain=-3.0:3.0, samples=250]
					plot ({\x},{\a*\x*\x + \c});
					
					% --- Dann den Abschnitt zwischen xA und xB in GRÜN überzeichnen ---
					% (für nach oben geöffnete Parabel liegt dieser Abschnitt unterhalb der Geraden)
					\draw[very thick, green!50!black, domain=\xA:\xB, samples=200]
					plot ({\x},{\a*\x*\x + \c});
					
					% Lilafarbene Sehne
					\draw[line width=2pt, purple!60!white] (\xA,\yA) -- (\xB,\yB);
					
					% Rote Punkte + Labels
					\fill[red!70!black] (\xA,\yA) circle (2.2pt)
					node[left=3pt, text=red!70!black] {$x$};
					\fill[red!70!black] (\xB,\yB) circle (2.2pt)
					node[right=3pt, text=red!70!black] {$y$};
					
				\end{tikzpicture}				
			\end{center}
			\item f is strictly cvx if the inequality holds strictly.
		\end{itemize}
		
	\end{definitionbox}
	
	\paragraph{Remarks}
	\begin{itemize}
		\item The definition extends to vector functions $f:C\to\mathbb{R}^n$ for convex $f_i$
		\item $f: C_1 \cross C_2 \to \mathbb{R}$\\
		$f(x,y)$ is jointly cvx, in $x,y$ if $z\coloneqq\begin{bmatrix}x\\y\end{bmatrix},\,f(z)$ is in cvx in $z$.
	\end{itemize}
		
	
	
	\paragraph{Example}
		$f(x,y) = x^2+y^2,\,z=\begin{bmatrix}x\\y\end{bmatrix}\to f(z) = z_1^2+z_2^2$
	
	\begin{definitionbox}
		An NLP is a \underline{convex program} if
		\begin{itemize}
			\item $f$ is convex function,
			\item $\mathcal{D}$ is convex set.
		\end{itemize}
	\end{definitionbox}
	
	\begin{lemmabox}
		Let $x^\star$ be a local minimizer of cvx program.  
		Then $x^\star$ is also global minimizer.
	\end{lemmabox}
	
	\textbf{Proof:} try as an erxercise\\
	
	Minimizers of convex NLP form a convex set.\\
	This set might be empty (Convex NLPs not guaranteed to have solution).\\
	\underline{However}: Unique solution for \underline{strictly} convex NLPs, if a solution exists.
	
	\begin{lemmabox}[First/Second order conditions for convexity]
		\begin{enumerate}
			\item $f:C\to\mathbb{R}$ continuously differentiable on $C$. Then $f$ is cvx iff
			\[
				f(y)\geq f(x)+\nabla f(x)^T(y-x), \quad\forall x,y\in C
			\]
			\[
				(\nabla f)_i = \frac{\partial f}{\partial x_i} \text{ is gradient (sometimes $f_{x_i}$)}
			\]
			\item $f$ twice differentiable on $C$, then $f$ convex iff
			\[
				\nabla_{xx}^2 f(x) \succeq 0 \quad \forall x \in C
			\]
			\[
				(\nabla_{xx}^2)_{i,j} = \frac{\partial^2 f}{\partial x_i \partial x_j} \quad \text{(Hessian)}
			\]
		\end{enumerate}
	\end{lemmabox}
	
	\begin{itemize}
		\item $A\succeq0$ means that: $A=A^T$ and pos semi-definite, i.\,e. all eigenvalues non-negative
		\item $f$ strictly cvx if $\nabla_{xx}^2 f(x) \succ 0 \quad \forall x \in C$ with $A\succ0$ meaning pos definite and symmetric
		\item Interpretation: Curvature of function should be non-negative/positive
		\item For exercises to check convexity, the second condition is generelly useful. First condition is useful for proofs.
	\end{itemize}
	
	For $\mathcal{D} = \{ x \mid g(x) \leq 0, \, h(x) = 0 \}$ the following holds: If
	
	\begin{minipage}{0.65\textwidth}
		\begin{itemize}
			\item $g$ are convex functions,
			\item $h$ are affine functions (i.e. $h(x) = 0 \Leftrightarrow Ax=b$),
		\end{itemize}
	\end{minipage}
	\begin{minipage}{0.15\textwidth}
		$\left.\vphantom{\begin{array}{l}g\\h\end{array}}\right\}$ sufficient
	\end{minipage}
	
	
	then $\mathcal{D}$ is a convex set.
	
	\paragraph{Example} $a,b\in\mathbb{R}$
	
	\begin{table}[H]
		\centering
		\begin{tabular}{c c c}
			$\underset{x}{\min}f$ & & $\underset{x}{\min}f$\\
			s.t. $x^3-1\leq0$ & $\qquad\leftrightarrow\qquad$ & s.t. $x-1\leq0$\\
			$(ax+b)^2=0$ & & $ax+b=0$\\
			non-convex & & convex\\
			non-affine & & affine
		\end{tabular}
	\end{table}
	
	\underline{Moral to recognize convexity of NLP:}
	\begin{enumerate}
		\item Use definition of cvx NLP, cvx $f$, convex $\mathcal{D}$
		\item If $\mathcal{D}$ written as equality/inequality-constraints, check $g$ convex/$h$ affine.\\
		If not, check further whether the feasible set is cvx or not (e.g. can be written equivalently with cvx $g$/affine $h$).
	\end{enumerate}
	
	\subsection{Unconstrained Problems}
	\[
		\mathcal{D} = \mathbb{R}^n
	\]
	Assume throughout that $f \in \mathcal{C}^1$ (continuously differentiable).
	
	\begin{definitionbox}[Descent Direction]
		$d \in \mathbb{R}^n$ is a \underline{descent direction} for $f$ at $\bar{x} \in \mathbb{R}^n$ if
		\[
		\exists \delta > 0 \quad \text{s.t.} \quad f(\bar{x} + \lambda d) < f(\bar{x}) \quad \forall \lambda \in (0, \delta).
		\]
		$F(\bar{x})$: \underline{Cone of descent directions}\\
		Set of all descent directions of $f$ at $\bar{x}$
	\end{definitionbox}
	
	A set $K \subseteq \mathbb{R}^n$ is a cone if it contains the full ray through any point in the set.
	
	\[
		K \text{ cone if }\forall x\in K \text{ and } \rho \geq 0, \quad \rho x \in K
	\]
	\begin{center}
		\begin{tikzpicture}[line cap=round,line join=round,>=stealth,thick,scale=1]
			
			% Achsen
			\draw[->] (-0.2,0) -- (5,0);
			\draw[->] (0,-0.2) -- (0,4);
			
			% Parameter
			\pgfmathsetmacro{\mA}{0.3}   % untere schwarze Linie
			\pgfmathsetmacro{\mB}{0.8}   % obere schwarze Linie
			\pgfmathsetmacro{\mR}{0.6}   % rote Mittellinie
			\pgfmathsetmacro{\L}{4.5}    % Länge
			
			% Koordinaten
			\coordinate (A) at (\L,\mA*\L);
			\coordinate (B) at (\L,\mB*\L);
			\coordinate (R) at (\L,\mR*\L);
			
			% Gelber Bereich zwischen den schwarzen Linien
			\path[fill=yellow!50,opacity=0.6] (0,0) -- (A) -- (B) -- cycle;
			
			% Schwarze Begrenzungslinien
			\draw[thick] (0,0) -- (A);
			\draw[thick] (0,0) -- (B);
			
			% Rote Mittellinie
			\draw[red,very thick] (0,0) -- (R);
			
			% Punkt x auf der roten Linie
			\coordinate (P) at (2.5, {2.5*\mR});
			\fill[black] (P) circle (2pt);
			\node[below right,blue,scale=1.3] at (P) {$x$};
			
		\end{tikzpicture}
		
	\end{center}	
	This is a geometric characterization of descent direction. It gives us a geometric condition for $x^\star$ to be a local minimizer.
	
	\begin{lemmabox}[Geometric Condition for local minimum]
		$x^\star$ is a local minimizer iff
		\[
		\mathcal{F}(x^\star) = \emptyset.
		\]
	\end{lemmabox}
	
	We want an algebraic condition to be able to compute or look for $x^\star$.
	
	\begin{lemmabox}[Algebraic first-order characterization of $\mathcal{F}$]
		If $\nabla f(\bar{x}) \neq 0$, then
		\[
		\mathcal{F}_0(\bar{x}) = \{ d \mid \nabla f(\bar{x})^T d < 0 \} = \mathcal{\mathcal{F}}(\bar{x}).
		\]
		Otherwise
		\[
		\mathcal{F}_0(\bar{x}) \subseteq \mathcal{F}(\bar{x}).
		\]
	\end{lemmabox}
	
	\textbf{Proof:} try Taylor-series expansion of $f$ at $\bar{x}$
	
	Graphical interpretation:
	
	
	
	$\nabla f$ forms angles greater or equal than $90^\circ$ with \underline{all} descent directions.
	
	\begin{lemmabox}[First-order necessary condition for local minimum]
		If $x^\star$ is a local minimizer, then
		\[
		\underset{\text{``stationary point''}}{\underbrace{\nabla f(x^\star) = 0}}.
		\]
	\end{lemmabox}
	
	\textbf{Proof:} Contradiction\\
	If $\nabla f(x^\star) \neq 0$, then $d = -\nabla f(x^\star) \neq 0$. Therefore there exists a descent direction $d\in\mathcal{F}(x^\star)$ by Lemma 1.4. Thus $\exists \delta > 0$ s.t. $f(x^\star+\lambda d)<f(x^\star) \quad \forall\lambda\in (0,\delta).$\\
	This is a contradiction with the fact, that $x^\star$ is a minimizer.\hfill\qedsymbol{}
	
	Why only necessary?\\
	It can't be a sufficient condition because in case where $\nabla f(x^\star) = 0$ we cannot use Lemma 1.4, e.g. $f_1(x) = -x^2,\,f_2(x) = x^3,\,\nabla f_1(0) = \nabla f_2(0) = 0.$
	
	\begin{lemmabox}[second order necessary condition]
		Assume f is twice continuously differentiable $f \in \mathcal{C}^2$
		\[
			x^\star\text{ local minimizer }\Rightarrow \nabla_{xx}^2 f(x^\star)\succeq0
		\]
	\end{lemmabox}
	
	\textbf{Note:} the condition on the Hessian of f can be interpreted as a local convexity property (around $x^\star)$.
	
	\textbf{Proof:} $2^{nd}$ order Taylor expansion around $x^\star$ in direction $d \in \mathbb{R}^n$:
	\[
		f(x^\star + \lambda d) = f(x^\star) + \lambda\nabla f(x^\star)^Td + \frac{\lambda^2}{2}d^T \nabla_{xx}^2 f(x^\star) d + \lambda^2\norm{d}^2\alpha (\lambda d)
	\]
	\[
		(\to\alpha(\cdot)) \text{ is a function that is order $1$ or higher in $\lambda d$})
	\]
	\begin{enumerate}
		\item If $x^\star$ is local minimzer $\Rightarrow\nabla f(x^\star) = 0$
		\item Divide by $\lambda^2$:
		\[
			\frac{f(x^\star+\lambda d)-f(x^\star)}{\lambda^2} = \frac{1}{2} d^T \nabla_{xx}^2 f(x^\star) d + \norm{d}\alpha (\lambda d)
		\]
		\item $\lambda \to 0$ on the right-hand-side the first term dominates
		\[
			\frac{f(x^\star+\lambda d)-f(x^\star)}{\lambda^2} \approx \frac{1}{2} d^T \nabla_{xx}^2 f(x^\star) d
		\]
		\item For $x^\star$ is a local minimizer, the left-hand-side must be $\geq 0$ for any $d\in\mathbb{R}^n$\\$\hspace*{1cm}\Rightarrow\quad d^T \nabla_{xx}^2 f(x^\star)d\geq0\quad\forall d\quad \Rightarrow\quad \nabla_{xx}^2 f(x^\star)\succeq0$\hfill\qedsymbol{}
	\end{enumerate}
	
	Only a necessary condition, because when $\nabla_{xx}^2 f(x^\star)$ is singular, we need to use higher-order information.
	
	Generally it is hard to get (global) sufficient conditions. $\to$ convexity to the rescue!
	
	\begin{lemmabox}[First order N\&S condition for global minimizers]
		Assume f is convex.
		\[
			\exists x^\star \text{ s.t. } \nabla f(x^\star) = 0 \quad\Leftrightarrow\quad x^\star \text{ is a global minimizer}
		\]
		If $f$ is  strictly convex, then the minimizer is unique.
	\end{lemmabox} 	
	
	\textbf{Proof:} $(\nabla f = 0\,\Rightarrow\,\text{global minimum})$
	
	First order condition for convexity:
	\[
		f(y) \geq f(x) + \nabla f(x)^T(y-x),\quad\forall x,y\in\mathbb{R}^n
	\]
	Pick $x = x^\star$:
	\[
		f(y) \geq f(x^\star),\quad\forall y\in\mathbb{R}^n
	\]
	(Other direction holds because of Lemma 1.5)
	
	What if we do not have global convexity?
	
	\begin{lemmabox}[Second order sufficient condition for local minimizer]
		Assume $f \in\mathcal{C}^2$.\\
		If $\nabla f(x^\star) = 0$ and $\nabla_{xx}^2 f(x^\star)\succ0\quad\Rightarrow\quad x^\star$ is strict local minimizer.
	\end{lemmabox}
	
	\textbf{Proof:} Taylor expansion (Similar to Lemma 1.6)
	
	\subsection{Constrained Problems}
	
	\[
		\mathcal{D} \subseteq\mathbb{R}^n,\quad\mathcal{D} = \{x \mid g_i(x)\leq0,\,i=1,\dots,n_g\quad h_j(x)=0,\,j=1,\dots,n_h\}
	\]
	
	We assume throughout $g_i,\,h_j$ are all $\mathcal{C}^1$ functions.
	
	\begin{definitionbox}[Tangent vector, tangent cone]
		$p\in\mathbb{R}^n$ is a tangent vector to $\mathcal{D}$ at $\bar{x}\in\mathcal{D}$ if $\exists$ differential curve $\bar{x}(s):[0,\varepsilon)\to\mathcal{D}$ with $\varepsilon>0$ such that $\bar{x}(0)=\bar{x},\left.\frac{d\bar{x}}{ds}\right|_{s=0}=p$.\\\\
		Tangent cone $\mathcal{T}_\mathcal{D}(\bar{x})$ to $\bar{x}$ is the set of all tangent vectors
		\[
			\mathcal{T}_\mathcal{D}(\bar{x})\coloneqq\{p\mid p \text{ tangent vector to $\mathcal{D}$ at $\bar{x}$}\}
		\]
	\end{definitionbox}
	
	Graphical representation:
	
	Set of directions that make us stay feasible (at least infinitesimally)
	
	When it comes to geometric conditions for optimality in constrained problems, we now have 2 sets/2 directions:
	\begin{itemize}
		\item $d\in\mathcal{F}(x)\to$ descent direction: objective improves
		\item $d\in\mathcal{T}_D(\bar{x})\to$ tangent vector: we stay feasible
	\end{itemize}
	
	\begin{lemmabox}[Geometric condition for local minimizer, $\mathcal{D}\subseteq\mathbb{R}^n$]
		$x^\star$ is a local minimizer iff $\mathcal{F}(x^\star)\cap\mathcal{T}_\mathcal{D}(x^\star)=\emptyset$
	\end{lemmabox}
	
	It is basically says that ``any improving direction can't be feasible''.
	
	
	
	As in the unconstrained case, we want to turn geometric conditions to algebraic ones.
	
	\begin{lemmabox}[$1$st order Nec. condition - semi-algebraic]
		If $x^\star$ is a local minimizer. Then:
		\begin{enumerate}
			\item $x^\star\in\mathcal{D}$
			\item $\underset{\text{geometric}}{\underline{\forall p\in\mathcal{T}_\mathcal{D}(x^\star)}}$, it holds $\underset{\text{algebraic}}{\underline{p^T\nabla f(x^\star)\geq 0}}$
		\end{enumerate}
	\end{lemmabox}
	
	\textbf{Proof:}
	
	Item 1 $\to$ feasibility
	
	Item 2: Assume there is a $p$ s.t. $p^T\nabla f(x^\star)<0$. Then 
	\[
		\exists\text{ curve }\bar{x}(s)\in\mathcal{D}\text{ s.t. }\left.\frac{df(\bar{x})}{ds}\right|_{s=0} \underset{\rotatebox[origin=c]{90}{$\Rsh$}\text{ chain rule }}{=p^T\nabla f(x^\star})<0
	\]
	which would mean that $p$ is descent direction. Contradicts $x^\star$ local minimizer.
	
	This is almost a translation of Lemma 1.9 because we replaced $\mathcal{F}(x^\star)$ with its \underline{algebraic} form ``$d^T\nabla f(x^\star)<0$''.
	
	To obtain a fully algebraic test, we need a few more concepts.
	
	\begin{definitionbox}[Active constraints, active set, regular points]
		$\bar{x}\in\mathcal{D}$
		\begin{itemize}
			\item $g_i$ is \underline{active} at $\bar{x}$ if $g_i(\bar{x})=0$
			\item $A(\bar{x}) = \{i\mid g_i(\bar{x})=0\}$ \underline{set of active constraints} at $\bar{x}$
			\item $\bar{x}\in\mathcal{D}$ is a \underline{regular point} if $\nabla g_i(\bar{x}),\,i\in\mathcal{A}(\bar{x})$ and $\nabla h_j(\bar{x}),\,j=1,\dots,n_h$ are linearly independent.
		\end{itemize}
	\end{definitionbox}
	
	\begin{lemmabox}[Algebraic first-order characterization of target set]
		If $\bar{x}$ is regular point. Then
		\[
			\mathcal{T}_\mathcal{D}(\bar{x})=\{p\mid\nabla h(\bar{x})^p=0,\,\nabla g_i(\bar{x})^p\leq0,\quad\forall i\in\mathcal{A}(\bar{x})\}\qquad\textcircled{1}
		\]
		where $\nabla h(\bar{x})\coloneqq[\nabla h_1(\bar{x}),\dots,\nabla h_{n_k}(\bar{x})]\in\mathbb{R}^{n\cross n_h}$.
		
		\textcircled{1} can be written equivalently as $\mathcal{T}_\mathcal{D}(\bar{x})=\{p\mid\mathcal{A}(\bar{x})p\geq0\}$
	\end{lemmabox}
	
	\[
	A(\bar{x})\coloneqq\left[
	\begin{array}{c}
		\nabla h(\bar{x})^T \\[4pt]
		-\nabla h(\bar{x})^T \\[4pt]
		\vdots \\[4pt]
		-\nabla g_i(\bar{x})^T \\[4pt]
		\vdots
	\end{array}
	\right]
	\begin{array}{l}
		\in\mathbb{R}^{(2n_h+\abs{\mathcal{A}(\bar{x})})\cross n} \\[6pt]
		\left.\rule{0pt}{3.0em}\right\} i\in\mathcal{A}(\bar{x})
	\end{array}
	\]
	
	In other words, item 2 of Lemma 1.10 can be written as follows:
	\[
		p\in\mathbb{R}^n:\, \mathcal{A}(x^\star)p\geq0,\,p^T\nabla f(x^\star)<0
	\]
	still not very tractable?
	
	Farkas Lemma to the rescue:
	
	\begin{lemmabox}[Farkas Lemma]
		For any matrix $A\in\mathbb{R}^{m\cross n}$, vector $b\in\mathbb{R}^n$.
		
		Exactly one of the following holds:
		\begin{enumerate}
			\item $\exists y\in\mathbb{R}^m\,y\geq0$, such that $A^Ty=b$
			\item $\exists p\in\mathbb{R}^m$, such that $A^Tp\geq0,\,p^Tb<0$
		\end{enumerate}
	\end{lemmabox}
	
	Take $A\equiv A(x^\star)$ and $b\equiv\nabla f(x^\star)$
	
	If we find $y$ satisfying 1., then 2. can't hold $\Rightarrow$ item 2 of Lemma 1.10 is verified $\Rightarrow$ $x^\star\in\mathcal{D}$ is a local minimizer.
	
	KTK-conditions just follow from imposing
	
	\hspace*{1cm}item 1 of Lemma 1.10 $\to$ $x^\star\in\mathcal{D}$
	
	\hspace*{1cm}item 2 of Lemma 1.10 $\to$ $\exists y\in\mathbb{R}^m,\,y\geq0$ s.t. $\mathcal{A}(x^\star)^Ty=\nabla f(x^\star)$
	
	\textbf{\underline{Conceptual summary}:}\nopagebreak
	
	\begin{tikzpicture}[
		node distance=2.5cm and 5cm,
		box/.style={draw, rectangle, rounded corners, align=center, minimum width=4cm, minimum height=1.2cm},
		arrow/.style={-{Latex[length=3mm]}, thick}
		]
		
		% Nodes
		\node[box] (lemma19) {Lemma 1.9\\ \textit{exact geometric}\\ \textit{characterization of $x^\star$}};
		\node[box, right=of lemma19] (lemma110) {Lemma 1.10\\ \textit{semi-algebraic}\\ \textit{ necessary condition}};
		\node[box, below=of lemma110] (kkt) {KKT conditions\\ \textit{(1$^{st}$ order necessary conditions)}};
		
		% Arrows
		\draw[arrow] (lemma19) -- node[above, sloped, align=center] {algebraic expression\\ for $\mathcal{F}(x^\star)$} (lemma110);
		\draw[arrow] (lemma110) -- node[left, align=left]{- algebraic expression of $\mathcal{T}_\mathcal{D}(x^\star)$\\ - Farkas Lemma} (kkt);
	\end{tikzpicture}
	
	\textbf{\underline{Informal recap}:}
	
	$x^\star$ local minimizer $\Rightarrow$ $x^\star\in\mathcal{D},\quad\forall p\in\mathcal{T}_D(x^\star),\quad p^\mathcal{T}\nabla f(x^\star)\geq0$
	
	\hspace*{8cm}$\Updownarrow$ (if $x^\star$ regular point)
	
	$\exists y=\left[
	\begin{array}{c}
		\hat{\lambda}_1 \\[4pt]
		\hat{\lambda}_2 \\[4pt]
		\vdots \\[4pt]
		-\nu_i \\[4pt]
		\vdots
	\end{array}
	\right]
	\begin{array}{l}
		\in\mathbb{R}^{2n_h+\abs{\mathcal{A}(x^\star)}} \\[15pt]
		\left.\rule{0pt}{3.0em}\right\} i\in\mathcal{A}(\bar{x})
	\end{array}\qquad y\geq0\quad\to A(x^\star)^Ty=\nabla f(x^\star)$
	
	\rule{\textwidth}{1pt}
	
	Let's write down $A^Ty=\nabla f$
	\[
		\nabla h(x^\star)(\hat{\lambda}_1-\hat{\lambda}_2)-\sum_{i\in\mathcal{A}(x^\star)}\nabla g_i(x^\star)\nu_i=\nabla f(x^\star),\quad\hat{\lambda}_1,\hat{\lambda}_2,\nu_i\geq0:\text{ But }(\hat{\lambda}_1-\hat{\lambda}_2)\gtreqless0
	\]
	Equivalently: $\lambda\coloneqq-(\hat{\lambda}_1-\hat{\lambda}_2)\in\mathbb{R}^{n_h}$, sign undefined
	\[
		\exists\lambda\in\mathbb{R}^{n_h},\quad\nu\in\mathbb{R}^{n_g},\quad\nu\geq0,\quad\nu_i=0,\quad i\notin\mathcal{A}(x^\star)
	\]
	\[
		\text{s.t. }\nabla f(x^\star)+\nabla h(x^\star)\lambda+\nabla g(x^\star)\nu=0
	\]
	\[
		\text{with }\nabla g(x^\star)\coloneqq\begin{bmatrix}
			\nabla g_1(x^\star)&\nabla g_2(x^\star)&\cdots&\nabla g_{n_g}(x^\star)
		\end{bmatrix}
	\]
	\[
	\text{and }\nabla h(x^\star)\coloneqq\begin{bmatrix}
		\nabla h_1(x^\star)&\nabla h_2(x^\star)&\cdots&\nabla h_{n_h}(x^\star)
	\end{bmatrix}
	\]
	
	We are now ready for a fully algebraic characterization.
	
	\begin{definitionbox}[Karash-Kuhn-Tucker (KKT) points]
		A triplet of vectors $(\bar{x},\bar{\lambda},\bar{\nu})\in\mathbb{R}^n\cross\mathbb{R}^{n_h}\cross\mathbb{R}^{n_g}$
		
		\hspace*{2cm}$\bar{x}:\text{ opt. variable},\quad\bar{\lambda}:\text{ multiplier equality constraints},$
		
		\hspace*{2cm}$\bar{\nu}:\text{ multiplier inequality constraints}$
		
		is a KKT point if
		\begin{enumerate}
			\item $\nabla f(\bar{x})+\nabla h(\bar{x})\bar{\lambda}+\nabla g(\bar{x})\bar{\nu}=0$
			\item $g(\bar{x})\leq0$
			\item $h(\bar{x})=0$
			\item $\bar{\nu}\geq0$
			\item $\bar{\nu}^Tg(\bar{x})=0$
		\end{enumerate}
	\end{definitionbox}
	
	\begin{lemmabox}[KKT necessary condition for local minimizer]
		If $x^\star$ is a local minimizer \textbf{and} a regular point.
		
		Then $\exists\lambda^\star,\nu^\star$ s.t. $(x^\star,\lambda^\star,\nu^\star$ is a KKT point)
	\end{lemmabox}
	
	\textbf{Proof:} Corollary of previous discussion
	\begin{enumerate}
		\item $\Longleftrightarrow\exists\lambda\in\mathbb{R}^{n_h},\,\nu\in\mathbb{R}^{n_g}$ s.t. $\nabla f(x^\star)+\nabla h(x^\star)\lambda+\nabla g(x^\star)\nu=0$ It can be written
		
		\hspace*{2em} equivalently as
		\[
			\left.\nabla_x\mathcal{L}(x,\lambda,\nu)\right|_{x=x^\star,\lambda=\lambda^\star,\nu=\nu^\star}=0
		\]
		where $\mathcal{L}(x,\lambda,\nu)\coloneqq f(x)+\lambda^Th(x)+\nu^Tg(x)$
		\item $\Longleftrightarrow x^\star\in\mathcal{D}$
		\item $\Longleftrightarrow x^\star\in\mathcal{D}$
		\item $\Longleftrightarrow$ non-negativity of ``$y$'' from Farkas Lemma
		\item $\Longleftrightarrow\nu_i=0,\,i\notin\mathcal{A}(x^\star)$
		
		
		\hspace*{2em} $\nu^T g(x^\star) = \sum_i \nu_i g_i(x^\star) = 0\quad$ Complementary slackness
		
		\hspace*{2em} $g_i(x^\star) = 
		\begin{cases}
			=0, & i \in \mathcal{A}(x^\star) \\
			<0, & i \notin \mathcal{A}(x^\star)
		\end{cases}
		\hspace*{0.5em} \text{because } x^\star \in \mathcal{D}$
		
		\hspace*{2em} $\nu_i \ge 0, \; \forall i\quad$ because of Farkas' lemma.
		
		Then $\sum_i \nu_i g_i(x^\star)$ automatically sets 
		$\nu_i = 0$ when $i \notin \mathcal{A}(x^\star)$ or $g_i(x^\star) < 0$.\hfill\qedsymbol{}
	\end{enumerate}
	
	Intrestingly, if NLP is convex, KKT conditions are sufficient for global optimality:
	
	\begin{lemmabox}[KKT sufficient conditions fot global minimizer]
		Suppose $f,g_i\quad(i=1,\dots,n_g)$ are convex functions and
		
		\hspace*{2cm}$h_j\quad(j=1,\dots,n_h)$ are affine functions.
		
		If $(x^\star,\lambda^\star,\nu^\star)$ is a KKT point, then $x^\star$ is a local minimizer.
	\end{lemmabox}
	
	\textbf{Proof:} For $(\lambda^\star,\nu^\star)$ KKT points:
	
	\[
		b(x)\coloneqq\mathcal{L}(x,\lambda^\star,\nu^\star)=f(x)+\sum_{i=1}^{n_g}\nu_i^\star g_i(x)+\sum_{j=1}^{n_h}\lambda_j^\star g_j(x)\hspace*{2cm} \textcircled{$\cross$}
	\]
	$f,g_i,h_j$ are convex functions
	
	Linear combination of cvx functions with non-negative coefficients is a convex function
	
	\hspace*{2cm}$\Rightarrow$ $b(x)$ convex
	
	\begin{enumerate}
		\item $b(x)$ convex
		\item $\nabla b(x^\star)=0$ because of $1.$, $(x^\star,\lambda^\star,\nu^\star)$ is a KKT point $b(x)\geq b(x^\star)\quad\forall x\in\mathbb{R}^n$
		
		\hspace*{5cm}$\Updownarrow$ (if $x^\star$ regular point)
		
		\[
			f(x)-f(x^\star)\geq-\underset{\leq0}{\underbracket{\sum_{i\in\mathcal{A}(x^\star)}\nu_i^\star g_i(x)}}-\underset{=0}{\underbracket{\sum_{j=1}^{n_h}\lambda_i^\star h_i(x)}}\geq0,\qquad x\in\mathcal{D}
		\]
		
		\hspace*{2.8cm}because $g(x)\leq0,\,\nu^\star\geq0$\hspace*{0.8cm}because $h(x)=0$
		
		$\to$ $x^\star$ is a global minimizer.\hfill\qedsymbol{}
	\end{enumerate}
	
	\underline{\textbf{Second-order conditions}}
	
	Similary to the unconstrained case, we can use the Hessian.
	
	\[
		\nabla_{xx}^2\mathcal{L}
	\]
	
	We need to check positive semi-definiteness of the Hessian only along feasible directions:
	
	Precisel, we are interestes in this property along
	
	\[
		\text{Critical Directions}=\{p\mid \underset{\text{feasible directions}}{\underbrace{p\in\mathcal{T}_\mathcal{D}(x^\star)}},\quad\underset{\substack{\text{directions that cannot}\\\text{be excluded based on}\\\text{on first order arguments}}}{\underbrace{\nabla f(x^\star)^Tp=0}}\}
	\]
	
	\[
	\left\{
	\begin{aligned}
		\nabla f(x^\star)^T p &< 0 \quad \to \quad p \text{ descent direction: }\substack{\text{already excluded by necessary}\\\text{condition of order 1}},\\[0.3em]
		\nabla f(x^\star)^T p &> 0 \quad \to \quad p \text{ ascent direction: ``not harmful''},\\[0.3em]
		\nabla f(x^\star)^T p &= 0 \quad \to \quad \text{this is what is ``new'' compared to first order.}
	\end{aligned}
	\right.
	\]
	
	\begin{lemmabox}[Second order necessary condition]
		\begin{itemize}
			\item $f, g, h \in \mathcal{C}^2$ at $x^\star$
			\item $x^\star$ local minimizer and regular point
			\item $(x^\star, \lambda^\star, \nu^\star)$ KKT point (which exists by Lemma 1.13)
		\end{itemize}		
		
		Then 
		\[
			p^T \nabla_{xx}^2 L(x^\star, \lambda^\star, \nu^\star) p \ge 0\quad \text{(curvature non-negative along critical directions)}
		\]

		$\forall p \ne 0$ with
		
		\begin{itemize}
			\item $\nabla h(x^\star)^T p = 0$
			\item $\nabla g_i(x^\star)^T p \le 0 \quad\forall i\in\mathcal{A}(x^\star)$ with $\nu_i^\star = 0\mid p \in \mathcal{T}_\mathcal{D}(x^\star)$
			\item $\nabla g_i(x^\star)^T p = 0 \quad\forall i\in\mathcal{A}(x^\star)$ with $\nu_i^\star > 0\mid\nabla f(x^\star)^T p = 0$
		\end{itemize}
	\end{lemmabox}
	
	\begin{lemmabox}[Second order suffiecient conditions for local minimizer]
		If $(x^\star,\lambda^\star,\nu^\star)$ is a KKT point with
		
		\[
			p^T\nabla_{xx}\mathcal{L}(^\star,\lambda^\star,\nu^\star)p>0
		\]
		
		for same $p$ as in Lemma 1.15.
		
		Then $x^\star$ is a strict local minimizer.
	\end{lemmabox}
	
	\section{Calculus of Variations}
	
	Goal in OC: Find a function that maximizes a functional (function of function) subject \hspace*{2.3cm}to dynamic constraints
	
	In Chapter 1 we characterized solutions to optimization problems over vectors $(\mathbb{R}^n)$
	
	\[
		\min f(x) \text{ s.t. }x\in\mathcal{D}\subseteq\mathbb{R}^n,\qquad\text{Static problem}
	\]
	
	\begin{itemize}
		\item We should introduce ``time'' or ``stages'' in the problem
		\[
			\min_{x_1,\dots,x_N}\;\sum_{k=1}^Nf(k,x_k,x_{k-1})\qquad\text{N coupled stages}
		\]
		\[
			\text{s.t. }x_k\in\mathcal{D}_k\subseteq\mathbb{R}^n,\quad k=1,\dots,N,\quad x_0\text{ given}
		\]
		equivalent to: (loses structure)
		\[
			z=\begin{bmatrix}
				x_1\\\vdots\\x_N
			\end{bmatrix}\to\min_z\;p(z)\qquad z\in\mathcal{Z},\quad z\in\mathbb{R}^{n\cross N}
		\]
		\item Continuous-time description of dynamics
		
		From $N$ stages to continuous time by taking $\infty$ many stages:
		
		\begin{center}
			\begin{tikzpicture}[scale=1.1, >=Latex]
			
			% --- Linkes Diagramm: diskrete Variablen ---
			\begin{scope}
				% Achsen
				\draw[->] (0,0) -- (4.4,0) node[right] {$i$};
				\draw[->] (0,-0.3) -- (0,3) node[left] {$x$};
				
				% Punkte und horizontale Linien
				\foreach \x/\y in {1/1.3, 2/2.3, 3/2.0, 4/1.7}{
					\draw[thick] (\x,0) -- (\x,\y);
					\fill[blue] (\x,\y) circle (2pt);
					\draw[blue, dashed] (0,\y) -- (\x,\y);
				}
				
				% Achsenbeschriftungen
				\node[below] at (1,0) {$1$};
				\node[below] at (4,0) {$N$};
				
				% Blaue Labels
				\node[blue] at (-0.3,1.3) {$x_1$};
				\node[blue] at (-0.3,2.3) {$x_2$};
				\node[blue] at (-0.3,1.7) {$x_N$};
				
				% Text oben
				\node[red] at (2,3.4) {\small discrete variables};
				
				% Roter Intervalltext
				\draw[red, thick, decorate, decoration={brace, amplitude=5pt, mirror}] (1,-0.4) -- (2,-0.4)
				node[midway, below=4pt, red] {\scriptsize this interval must go to zero};
			\end{scope}
			
			
			% --- Pfeil zwischen den Diagrammen ---
			\draw[->, very thick] (4.7,1.5) -- (5.7,1.5);
			
			
			% --- Rechtes Diagramm: kontinuierliche Variable ---
			\begin{scope}[xshift=6.8cm]
				% Achsen
				\draw[->] (0,0) -- (3.8,0) node[right] {$s$};
				\draw[->] (0,-0.3) -- (0,3) node[left] {$x$};
				
				% Blaue Kurve (x(s))
				\draw[blue, thick, smooth]
				plot coordinates {(0.8,1.8) (1.8,2.6) (2.9,1.2) (3.3,1.4)};
				
				% Vertikale Linien bei s1 und s2
				\draw[dashed, blue] (0.8,0) -- (0.8,1.8);
				\draw[dashed, blue] (3.3,0) -- (3.3,1.4);
				
				% Horizontale Hilfslinien
				\draw[dashed, blue] (0,1.8) -- (0.8,1.8);
				\draw[dashed, blue] (0,1.4) -- (3.3,1.4);
				
				% Beschriftungen
				\node[below] at (0.8,0) {$s_1$};
				\node[below] at (3.3,0) {$s_2$};
				\node[blue] at (-0.5,1.9) {$x(s_1)$};
				\node[blue] at (-0.5,1.3) {$x(s_2)$};
				\node[blue] at (2.6,2.8) {$x(s)$};
				
				% Text oben
				\node[red] at (2,3.4) {\small continuous variables};
			\end{scope}
			
		\end{tikzpicture}
		\end{center}
		
		\[
		\min_{x(\cdot)} \; \int_{s_1}^{s_2} f\big(s, x(s), \dot{x}(s)\big) \, ds
		\]
		\[
		\text{s.t.} \quad
		\begin{aligned}
			&x(s) \in \mathcal{X} \subseteq \mathbb{R}^n, \quad s \in [s_1, s_2],\\
			&x(s_1) = x_1,
		\end{aligned}
		\quad
		\left\{
		\begin{array}{l}
			\text{prototypical CV problem}\\[4pt]
			\bullet\; \text{no ODE yet}\\[4pt]
			\bullet\; \text{opt. variable lives in a function space}
		\end{array}
		\right.
		\]
	\end{itemize}
	
	\subsection{Introduction to CV (Calculus of Variations)}
	
	Function CLASSES \& NORMS
	
	\[
		(\underset{\text{vector space}}{V},\underset{\text{norm}}{\norm{\cdot}})\quad\text{normed vector space}
	\]
	
	$V$ is the set of vector functions
	\[
		x(s),\quad s\in[s_1,s_2]\text{ taking values in }\mathbb{R}^n,\quad[s_1,s_2]\subseteq\mathbb{R}
	\]
	Two classes:
	\begin{itemize}
		\item $V = \mathcal{C}^1([s_1, s_2], \mathbb{R}^n)$: continuously differentiable functions $x: [s_1, s_2] \to \mathbb{R}^n$
		\item $\hat{V} = \hat{\mathcal{C}}^1([s_1, s_2], \mathbb{R}^n)$: piecewise continuously differentiable functions
		
		\hspace*{3.6cm} $x: [s_1, s_2] \to \mathbb{R}^n$.
	\end{itemize}
	
	\begin{definitionbox}[Piecewise continuously differentiable functions]
			$x: [s_1, s_2] \to \mathbb{R}^n$ is piecewise continuously differentiable (PCD) if
		\begin{itemize}
			\item $x \in \mathcal{C}\text{ on }[s_1, s_2]$,
			\item $\exists$ a finite partition $\{c_k\}_{k=0}^{N+1}$ with 
			\[
			s_1 = c_0 < c_1 < \dots < c_{N+1} = s_2,
			\]
			such that $x: [c_k, c_{k+1}] \to \mathbb{R}^n$ is $\mathcal{C}^1$.
			
			That is $x \in \mathcal{C}^1([c_k, c_{k+1}], \mathbb{R}^n), \quad \forall k = 0, 1, \dots, N$.
		\end{itemize}
	\end{definitionbox}
	
	\textbf{Example:}\nopagebreak\noindent
	\begin{center}
		\begin{tikzpicture}[scale=1.2, >=Latex]
			
			% Achsen
			\draw[->] (0,0) -- (6,0) node[right] {$s$};
			\draw[->] (0.5,-0.5) -- (0.5,2.8) node[left] {$x$};
			
			% Achsenbeschriftungen
			\node[below] at (1,0) {$s_1$};
			\node[below] at (2.3,0) {$c_1$};
			\node[below] at (3.8,0) {$c_2$};
			\node[below] at (5.2,0) {$s_2$};
			
			% Blaue Kurve: stückweise C^1 mit sichtbaren Knicken
			\draw[blue, thick, smooth, tension=0.9]
			plot coordinates {(1,0.0) (1.4,1.0) (1.9,0.8) (2.3,1.3)};
			% Knick an c1
			\draw[blue, thick, smooth, tension=0.8]
			plot coordinates {(2.3,1.3) (2.8,0.7) (3.3,1.0) (3.8,0.6)};
			% Knick an c2
			\draw[blue, thick, smooth, tension=0.8]
			plot coordinates {(3.8,0.6) (4.3,1.0) (4.7,1.4) (5.2,2.0)};
			
			% Eckenpunkte (rot)
			\filldraw[red] (2.3,1.3) circle (2pt);
			\filldraw[red] (3.8,0.6) circle (2pt);
			
			% Gelbe Markierungen um die Ecken
			\foreach \x/\y in {2.3/1.3, 3.8/0.6} {
				\draw[fill=yellow, opacity=0.3, draw=none] (\x,\y) circle (6pt);
			}
			
			% Rotes Intervall [c2, s2]
			\draw[line width=4pt, red!40, opacity=0.5] (3.8,0) -- (5.2,0);
			
			% Textbeschriftungen
			\node[blue] at (2.7,2.2) {$x(s) \in \hat{\mathcal{C}}^1$};
			
			\node[purple, align=left] (text1) at (6.4,1.0)
			{Inside the interval\\ it is a $\mathcal{C}^1$ function};
			\draw[purple, ->, thick] (text1.west) -- (4.5,0.1);
			
			\node[purple, align=center] (text2) at (3.0,-1.2)
			{2 corner points in this case};
			\draw[purple, ->, thick] (text2.north) ++ (-0.5,0.0) -- (2.3,-0.5);
			\draw[purple, ->, thick] (text2.north) ++ (0.5,0.0) -- (3.8,-0.5);
			
		\end{tikzpicture}
	\end{center}
	
	\subsubsection*{Norms}
	
	\begin{definitionbox}[Strong and weak norms]
		\underline{Case $V=\mathcal{C}^1$:}
		\begin{itemize}
			\item Strong norm (or $\infty$-norm)
			\[
				\norm{x}_\infty\coloneqq\underset{s_1\leq s\leq s_2}{\max}\norm{x(s)}_\text{  $\leftarrow$ any norm in $\mathbb{R}^n$}
			\]
			\item Weak norm (or $1$-norm)
			\[
				\norm{x}_1\coloneqq\norm{x}_\infty+\underset{s_1\leq s\leq s_2}{\max}\norm{\dot{x}(s)}_\text{  $\leftarrow$ any norm in $\mathbb{R}^n$}
			\]
			\[
				(\mathcal{C}^1([s_1,s_2]),\,\norm{\cdot})\text{ full notation}
			\]
		\end{itemize}
		Note $\forall x\in V, \norm{x}_1\geq\norm{x}_\infty$
		
		\underline{Case $V=\hat{\mathcal{C}}^1$:}
		\begin{itemize}
			\item Strong norm $\to$ same as for $\mathcal{C}^1$
			\item weak norm
			\[
				\norm{x}_1\coloneqq\norm{x}_\infty+\underset{s\in\bigcup\limits_{k=0}^N(c_k,c_{k+1})}{\sup}\norm{\dot{x}(s)}
			\]
		\end{itemize}
	\end{definitionbox}
	
	\subsubsection*{CV problem}
	
	\[
		\underset{x\in V}{\min}\;\underset{\rotatebox[origin=c]{180}{$\Lsh$}\text{ functional }J:V\to\mathbb{R}}{\underline{J}(x)\hspace*{2.2cm}}
	\]
	\[
		\text{s.t. }x\in\underset{\rotatebox[origin=c]{180}{$\Lsh$}\text{ admissible set}}{\underline{\mathcal{D}}\hspace*{2.1cm}}
	\]
	
	$\bar{x}\in\mathcal{D}$ admissible curve for trajectory.
	
	3 forms of $J$:
	\begin{itemize}
		\item Langrangian form
		\[
			J(x)\coloneqq\int_{s_1}^{s_2}\underset{\rotatebox[origin=c]{180}{$\Lsh$}\text{ running cost, }L:\mathbb{R}\cross\mathbb{R}^n\cross\mathbb{R}^n\to\mathbb{R}}{\underline{l}(s,x(s),\dot{x}(s))ds\hspace*{1.6cm}}
		\]
		\item Bolza form
		\[
			J(x)\coloneqq\varphi(s_2,x(s_2))+\int_{s_1}^{s_2}l(s,x(s),\dot{x}(s))ds\qquad\varphi:\mathbb{R}\cross\mathbb{R}^n\to\mathbb{R}
		\]
		\item Mayer form
		\[
			J(x)\coloneqq\varphi(s_2,x(s_2))
		\]
	\end{itemize}
	
	These 3 forms are (gernerally) interchangeble, e.g. $L\Rightarrow B,\; B\Rightarrow L$.
	
	We will see them again in Chapter 3.
	
	Admissible sets:
	
	\begin{itemize}
		\item Free problems $\to$ only endpoints are constrained. Example:
		\[
			\mathcal{D}=\{x\in V\mid x(s_1)=x_1,\; x(s_2)=x_2\},\qquad x_1,x_2\text{ fixed vectors}
		\]
		\item Isoperimetric constraints $\to$ level sets
		\[
			\mathcal{D}=\bigcap_{i=1}^{n_g}\Lambda_i(k_i)
		\]
		\[
			\Lambda_i(k_i)\coloneqq\{x\in V\mid\int_{s_1}^{s_2}g_i(s,x(s),\dot{x}(s))ds=k_i\}
		\]
	\end{itemize}
	
	\textbf{Example:}
	\[
		\underset{x}{\min}\; J(x)
	\]
	
	\[
	\begin{aligned}
		&\text{s.t.}\quad
		\left.
		\begin{aligned}
			&\displaystyle \int_{s_1}^{s_2} x^2(s)\,ds = 1\\[0.4em]
			&\displaystyle \int_{s_1}^{s_2} x(s)\,ds = 0
		\end{aligned}
		\right\}
		\quad
		\begin{array}{l}
			n_g = 2,\\[0.2em]
			g_1(s)=x^2(s),\\[0.2em]
			g_2(s)=x(s)
		\end{array}
	\end{aligned}
	\]
	
	\subsubsection*{Minimizers for CV problems}
	
	\begin{definitionbox}[Global and local minimizers]
		$x^\star\in\mathcal{D}$ is a global minimizer of [CV] if
		\[
			J(x)\geq J(x^\star),\qquad x\in\mathcal{D}
		\]
		
		$x^\star\in\mathcal{D}$ is a strong local minimizer of [CV] if
		
		\[
			\exists\varepsilon>0\;\text{s.t. } J(x)\geq J(x^\star),\quad\forall x\in B_\varepsilon^\infty(x^\star)\cap\mathcal{D}
		\]
		\[
			B_\varepsilon^\infty\coloneqq\{y\in V\mid\norm{x-y}_\infty\leq\varepsilon\}\quad\text{(strong ball)}
		\]
		$x^\star\in\mathcal{D}$ is a weak local minimizer of [CV] if
		\[
			\exists\varepsilon>0\;\text{s.t. }J(x)\geq J(x^\star),\quad\forall x\in B_\varepsilon^1(x^\star)\cap\mathcal{D}
		\]
		\[
			B_\varepsilon^1\coloneqq\{y\in V\mid\norm{x-y}_1\leq\varepsilon\}\quad\text{(weak ball)}
		\]
	\end{definitionbox}
	
	We will often call strong/weak minimizers (without local).
	
	\textbf{Note:} Every strong minimizer is a weak minimizer. In general, the converse is not true.
	
	Why?
	
	$\forall x\in V,\;\forall\varepsilon>0,\;B_\varepsilon^1(x)\subseteq_\varepsilon^\infty(x)$
	
	\begin{center}
		\begin{tikzpicture}[scale=1.4]
			
			% Farben
			\definecolor{lightyellow}{RGB}{255, 255, 153}
			
			% Gelb schraffierte Ringfläche
			\fill[lightyellow, opacity=0.5] (0,0) circle (2cm);
			\fill[white] (0,0) circle (1cm);
			
			% Kreise
			\draw[red, thick] (0,0) circle (2cm);
			\draw[blue, thick] (0,0) circle (1cm);
			
			% Mittelpunkt
			\filldraw (0,0) circle (1pt) node[below left] {$x$};
			
			% Pfeile für Radius
			\draw[blue,->] (0,0) -- ({1/sqrt(2)},{1/sqrt(2)}) node[midway, above] {$\varepsilon$};
			\draw[red,->] (0,0) -- ({2/sqrt(2)},{-2/sqrt(2)}) node[midway, below] {$\varepsilon$};
			
			% Labels positioniert
			\node[blue] at (0,1.3) {$B_\varepsilon^1(x)$};
			\node[red] at (2.5,0) {$B_\varepsilon^\infty(x)$};
			
		\end{tikzpicture}
	\end{center}
	\[
	\|x\|_1 \ge \|x\|_\infty
	\]
	
	Implication for CV is that a curve $x$ that is better than all elements in $B_\varepsilon^1(x)$ is not necessarily better than all elements in $B_\varepsilon^\infty(x)$.
	
	\textbf{Example:} $s_1=0,\;s_2=1$
	
	\[
		J(x)=\int_{0}^{1}\dot{x}^2(s)-\dot{x}^4(s)ds
	\]
	\[
		\mathcal{D}=\{x\in\hat{\mathcal{C}}^1([0,1])\mid x(0)=x(1)=0\}
	\]
	\[
		\bar{x}(s)=0,\quad J(\bar{x})=0,\quad\bar{x}\text{ is a weak minimum but not strong}
	\]
	\underline{Weak minimum}\nopagebreak
	
	$B_\varepsilon^1(0)$, take 0<$\varepsilon\leq1$
	\[
		\forall x\in B_\varepsilon^1(0),\;\norm{\dot{x}(s)}\leq\varepsilon\qquad\forall s\in[0,1]
	\]
	because x is in the weak ball with radius $\varepsilon$.
	\[
		J(x)=\int_{0}^{1}\underset{\geq0}{\underbrace{\dot{x}^2(s)}}(\underset{\geq0\text{ see above, }\varepsilon\leq1}{\underbrace{1-\dot{x}^2(s)}})ds\geq0,\quad\forall x\in B_\varepsilon^1
	\]
	$J(\bar{x})=0\to\bar{x}$ is a weak minimum, but not strong. Try to see why? $\to$ Find counterexamples.
	
	\textbf{Existence of solutions for problems of CV:}
	
	Weierstrass theorem still holds but compactness $\neq$ closed and bounded in function vector spaces.
	
	A subspace $\mathcal{D}$ of a metric space $V$ is compact, if ``every sequence in $\mathcal{D}$ has a subsequence converging to some point in $\mathcal{D}$''.
	
	\textbf{Example:}
	\[
		B_1^\infty(0)=\{x\in\mathcal{C}([0,1])\mid\norm{x}_\infty\leq1\}
	\]
	closed and bounded, \underline{not} a compact set.
	
	Bottom line: checking Weierstrass in CV can be overly restrictive because our common sets in $\mathcal{D}$ are not compact.
	
	Existence of solutions difficult to guarantee a-priory.
	
	Convexity is a special case where this is possible.
	
	\text{Variations:} (extension of perturbations to the cost seen in NLP)
	
	\begin{definitionbox}[First-variation of a functional]
		\underline{First variation} (Gateaux derivative) of $J$ at $x\in V$ in direction $\xi\in V$ is
		\[
			\delta J(x,\xi)\coloneqq\underset{\eta\to0}{\lim}\frac{J(x+\eta\xi)-J(x)}{\eta}=\left.\frac{\partial J(x+\eta\xi)}{\partial\eta}\right|_{\eta=0}
		\]
		\[
			J:V\to\mathbb{R}
		\]
	\end{definitionbox}
	$\delta J$ can be interpreted as follows
	\[
		J(x+\eta\xi)=J(x)+\eta\delta J(x;\xi)+\underline{o(\eta)}_{\text{ second order term }\underset{\eta\to0}{\lim}\;\frac{0(\eta)}{\eta}=0}
	\]
	$\delta J$ is functional associated with J and a point $x$ mapping a perturbation $\xi$ into a scalar, representing the variation of $J$ in that direction $\approx$ ``directional derivative'' for CV.
	
	\textbf{Example:}
	\[
		J(x)=\int_{s_1}^{s_2}x^2(s)ds,\quad V=\mathcal{C}^1
	\]
	$\delta J$? Apply defintion:
	\[
		\frac{J(x+\eta\xi)-J(x)}{\eta}=\frac{1}{\eta}\left[\int_{s_1}^{s_2}(x(s)+\eta\xi(s))^2-x^2(s)ds\right]
	\]
	\[
		=2\int_{s_1}^{s_2}x(s)\xi(s)ds+\eta\int_{s_1}^{s_2}\xi^2(s)ds
	\]
	\[
		\underset{\eta\to0}{\lim}\quad\longrightarrow\quad\delta J(x;\xi)=2\int_{s_1}^{s_2}x(s)\xi(s)ds
	\]
	
	From the defintion, we can see that $\delta J$ is a linear operator on V.
	\[
		\delta(J_1+J_2)(x;\xi)=\delta J_1(x;\xi)+\delta J_2(x;\xi).
	\]
	Moreover, it is a homogeneous operator: $\forall\alpha\in\mathbb{R}$ it holds 
	\[
		\delta J(x;\alpha\xi)=\alpha\delta J(x;\xi).
	\]
	
	\begin{definitionbox}[Second variation of a functional]
		\[
			\delta^2 J(x;\xi)\coloneqq\left.\frac{\partial^2}{\partial\eta^2}J(x+\eta\xi)\right|_{\eta=0}
		\]
	\end{definitionbox}
	Interpretation $J(x+\eta\xi)=J(x)+\eta\delta J(x;\xi)+\eta^2\delta^2J(x;\xi)+o(\eta^2)$
	
	Fundamental Lemma in CV.
	
	\begin{definitionbox}[Descent direction in CV]
		Given $V,\;J:V\to\mathbb{R}$ Gateaux-differentiable ($\delta J(x;\xi)$ exists) at $\bar{x}\in V$, we call $\xi\in V$ a descent direction for $J$ at $\bar{x}$ if
		\[
			\delta J(\bar{x};\xi)<0
		\]
	\end{definitionbox}
	There is a close connection with ``descent direction'' from NLP.
	
	\begin{definitionbox}[$\mathcal{D}$-admissible direction]
		Given $V,\;\mathcal{D}\subseteq V$ and $J:V\to\mathbb{R}$.
		
		$\xi\in V,\;\xi\neq0$ is $\mathcal{D}$-admissible for $J$ at $\bar{x}\in\mathcal{D}$ if
		\begin{itemize}
			\item $\delta J(\bar{x};\xi)$ exists
			\item $\exists\beta>0$ s.t. $\bar{x}+\eta\xi\in\mathcal{D},\quad\forall\eta\in(-\beta,\beta)$
		\end{itemize}
	\end{definitionbox}
	\textbf{Example:}
	\begin{center}
		\begin{minipage}{0.4\textwidth}
			\begin{tikzpicture}[scale=1.4]
				
				% Achsen
				\draw[->] (-0.5,0) -- (3,0) node[right] {$s$};
				\draw[->] (0,-0.5) -- (0,2.5) node[above] {$x$};
				
				% Punkte s_1 und s_2
				\filldraw[purple] (0.5,0) circle (2pt) node[below] {$s_1$};
				\filldraw[purple] (2.5,0) circle (2pt) node[below] {$s_2$};
				
				% \bar{x}
				\draw[thick] (0.5,0) .. controls (1.1,2.3) and (1.9,2.3) .. (2.5,0)
				node[midway, above] {$\bar{x}$};
				
				% \xi
				\draw[blue, thick]
				(0.5,0) .. controls (0.9,-0.5) and (1.7,0.7) .. (2.5,0)
				node[midway, above] {$\xi$};
				
			\end{tikzpicture}
		\end{minipage}
		\begin{minipage}{0.3\textwidth}
			\[
			\mathcal{D}:\;\bar{x}(s_1)=\bar{x}(s_2)=0
			\]
			\[
			\xi \text{ is } D\text{-admissible}
			\]
		\end{minipage}
	\end{center}
	
	\begin{lemmabox}[Negative result for minimizers in unconstrained CV]
		$(V,\norm{\cdot}),\;J$.
		
		Suppose at $\bar{x}\exists$ descent direction $\bar{\xi}\in V$.
		
		Then $\bar{x}$ cannot be a local minimizer for J (neither strong or weak).
	\end{lemmabox}
	\textbf{Proos:} Use definition of 1$^{st}$ variation.
	
	If $\delta J(\bar{x}_i\bar{\xi})<0$ then
	\[
		J(\bar{x}+\eta\bar{\xi})<J(\bar{x})\quad\forall\eta\in(0,\beta)
	\]
	This comes from
	\[
		J(x+\eta\xi)=J(x)+\eta\xi J(x;\eta)+o(\eta)
	\]
	Thus $\bar{x}$ can't be a local minimizer.
	
	\begin{lemmabox}[Geometric necessary condition for a local minimum, Fundamental Lemma of CV]
		$(V,\norm{\cdot}),\;\mathcal{D}\subseteq V,\;J\to\mathbb{R}$.
		
		Suppose $x^\star\in\mathcal{D}$ is a local minimizer for $J$ on $\mathcal{D}$, then
		\[
			\boxed{\delta J(x^\star;\xi)=0}\qquad \forall\mathcal{D}\text{-admissible directions at }x^\star
		\]
	\end{lemmabox}
	\textbf{Proof:} By contradiction:
	
	Case 1: $\delta J(x^\star;\xi)<0$
	
	\hspace*{1cm}By Lemma 2.1 $x^\star$ can't be a local minimizer $\to$ contradiction
	
	Case 2: $\delta J(x^\star;\xi)>0$
	
	\hspace*{1cm}By definition of the $\mathcal{D}$-admissible direction, if $\xi$ is $\mathcal{D}$-admissible.
	
	\hspace*{1cm}Then $-\xi$ is also $\mathcal{D}$-admissible.
	\[
		\delta J(x^\star;-\xi)=-\delta J(x^\star;\xi)<0\quad\text{(because of linearity)}
	\]
	$\to x^\star$ cannot be a local minimizer (because $\delta J(x^\star;-\xi)<0$ and $-\xi$ is $\mathcal{D}$-admissible). Thus $\delta J(x^\star;\xi)=0$.
	
	All the results in the rest of this chapter are $\underset{\substack{\text{they turn such an abstract}\\\text{requirenment into algebraic tests}}}{\text{``merely corollaries''}}$ of this Lemma.
	
	\subsection{Free problems of CV}
	
	\subsubsection{$V=\mathcal{C}^1$}
	\[
		\underset{x(\cdot)}{\min}\int_{s_1}^{s_2}l(s,x(s),\dot{x}(s))ds\hspace*{2cm}[CV-P_1]
	\]
	\[
		\text{s.t. }x\in\{y\in\mathcal{C}^1([s_1,s_2])\mid \underset{\substack{\text{special case of ``free problems''}\\\text{we fix the endpoint to }x_2}}{\underbrace{y(s_1)=x_1,\;y(s_2)=x_2}}\}
	\]
	
	\begin{lemmabox}
		Suppose $x^\star\in\mathcal{C}^1([s_1,s_2])$ is a weak minimum of $[CV-P_1]$.
		
		Then
		\[
			\frac{d}{ds}l_{\dot{x}_i}(s,x^\star(s),\dot{x}^\star(s))=l_{x_i}(s,x^\star(s),\dot{x}^\star(s)),\quad\forall s\in[s_1,s_2],\;i=1,\dots n
		\]
		with $l_{\dot{x}_i}=\frac{\partial l}{\partial\dot{x}_i}$ and $l_{x_i}=\frac{\partial l}{\partial x_i}$ (Euler equations).
	\end{lemmabox}
	This is a set of nonlinear ordinary time-varying second-order differential equations. Their solutions are \underline{candidate} local minimizers of $[CV-P_1]$.
	
	solution $x(s):\;s\in[s_1,s_2]$
	
	also called ``stationaty solutions'' of the corresponding CV problem. Why?
	\[
		\text{Because }\delta J(x^\star;\xi)=0\quad\forall\xi\;\mathcal{D}\text{-admissible}
	\]
	\textbf{Proof:} The idea is to derive algebraic conditions that are sufficient to guarantee $\delta J(x^\star; \xi) = 0$.
	
	First step is to write $\delta J:\;\eta\in\mathbb{R},\;\xi\in\mathcal{C}^1$
	$$ \frac{\partial}{\partial\eta} J(x^\star + \eta\xi) \underset{\text{Leibniz rule}}{=} \int_{s_1}^{s_2} \frac{\partial}{\partial \eta} l(s, x^\star + \eta\xi, \dot{x}^\star + \eta \dot{\xi})\, ds $$
	$$= \int_{s_1}^{s_2} l_x\left[x^\star+\eta\xi\right]^T\xi+ l_{\dot{x}}\left[x^\star+\eta\xi\right]^T\dot{\xi}\, ds$$
	with $l_z[y]\coloneqq l_z(s,y,\dot{y})$.
	
	Take $\eta\to0\quad\delta J(x^\star;\xi)=\int_{s_1}^{s_2}\underbracket{l_x\left[x^\star\right]^T+l_x\left[x^\star\right]^T\dot{\xi}}\,ds$
	
	\hspace*{4cm}integrand is a continuous function
	
	\hspace*{5cm}$\to$ first-variation exists $\forall\xi$ $\to$ $J$ is Gateaux-differentiable
	
	We want to obtain conditions enforcing $\delta J=0\;\forall\mathcal{D}$-admissible $\xi$. This means $\xi(s_1)=\xi(s_2)=0$ and $\xi\in\mathcal{C}^1$.
	\begin{center}
		\begin{tikzpicture}
			\begin{axis}[
				axis lines = middle,
				xlabel={$s$},
				ylabel={$x$},
				xmin=0, xmax=10,
				ymin=-3, ymax=6,
				xtick={2,8},
				xticklabels={$s_1$,$s_2$},
				ytick=\empty,
				width=12cm,
				height=6cm,
				axis line style={->},
				]
				
				% x^star
				\addplot[thick,blue,smooth]
				coordinates {
					(2,3) (3.5,4.5) (6,3.5) (8,5.5)
				}node [midway,above,blue] {$x^\star$};
				
				% xi
				\addplot[thick,blue,smooth]
				coordinates {
					(2,0) (3,2) (5,-2) (7,1.5) (8,0)
				}node [midway,above,blue] {$\xi$};
				
				% Tiny airplane mark — you can replace with your own symbol
				\node at (axis cs:9,4.8) {$\small$};
				
			\end{axis}
		\end{tikzpicture}
	\end{center}
	
	To do this, we select $n$ perturbations $\xi^{(i)}(i=1,\dots n)$ defined as follows
	$$\xi^{(i)}=\begin{bmatrix}
		\xi^{(i)}_1\\
		\vdots\\
		\xi^{(i)}_n
	\end{bmatrix}$$
	\begin{itemize}
		\item $\xi^{(i)}_j=0,\quad j\neq i$
		\item $\xi_i^{(i)}$ arbitrary but not identically zero with $\xi^{(i)}_i(s_1)=\xi^{(i)}_i(s_2)=0$.
	\end{itemize}
	
	We replace those $n$ perturbations in the equation $\delta J=0$
	$$
	\forall i \in \{1, \dots, n\}, \quad 0 = \delta J(x^\star, \xi^{(i)}) = \int_{s_1}^{s_2} \left[ l_{x_i}[x^\star]\xi_i + l_{\dot{x}_i}[x^\star]\dot{\xi}_i \right] \, ds.
	$$
	$$
	= \int_{s_1}^{s_2} l_{\dot{x}_i}[x^\star]\dot{\xi}_i \, ds + \int_{s_1}^{s_2} \underbrace{\frac{d}{ds} \left[ \left( \int_{s_1}^s l_{\dot{x}_i}[x^\star] d\sigma \right) \right]}_{v'} \underbrace{\dot{\xi}_i}_u \, ds
	$$
	integral by parts $\int_a^b u v' = [u v]_a^b - \int_a^b u' v$
	$$
	= \int_{s_1}^{s_2} l_{\dot{x}_i}[x^\star]\dot{\xi}_i \, ds + \underbrace{\left[ \xi_i \int_{s_1}^s l_{x_i}[x^\star] d\sigma \right]_{s_1}^{s_2}}_{=0 \text{ because } \mathcal{D}\text{-admissible}} - \int_{s_1}^{s_2} \left[ \int_{s_1}^s l_{x_i}[x^\star] d\sigma \right] \dot{\xi}_i \, ds
	$$
	$$
	=\int_{s_1}^{s_2} \left[ l_{\dot{x}_i}[x^\star] - \int_{s_1}^{s_2} l_{x_i}[x^\star] d\sigma \right] \dot{\xi}_i \, ds
	$$
	
	\textbf{DuBois-Reymond's Lemma}
	if 
	\begin{itemize}
		\item $h(s)$ continuous in $[s_1,s_2]$
		\item $\displaystyle \int_{s_1}^{s_2} h(s)\, \dot{y}(s)\, ds = 0$
		\item $y(s_1) = y(s_2) = 0$
	\end{itemize}
	$\Rightarrow \quad h(s)$ constant in $[s_1,s_2]$
	
	\bigskip
	
	This applies to our problem \quad $y \equiv \dot{\xi}_i$
	$$
	h \equiv L_{\dot{x}_i}[x^\star] - \int_{s_1}^{s} C_{x_i}[x^\star]\, d\sigma
	$$
	$$
	\Rightarrow \quad l_{\dot{x}_i}[x^\star] - \int_{s_1}^{s} l_{x_i}[x^\star]\, d\sigma = c_i,
	\qquad \forall s \in [s_1,s_2],\; i=1,\ldots,n,\; c_i : \text{ constants}
	$$
	
	Note that this shows $l_{\dot{x}_i} \in \mathcal{C}^1$ 
	
	What we obtain are the ``integral'' Euler equations (EE)
	
	If we take $\dfrac{d}{ds}$ we obtain the EE.
	
	\textbf{Remarks}
	\begin{enumerate}
		\item Necessary conditions for local minimizers ($\to$ weak)
		
		\begin{tikzpicture}[>=stealth, scale=0.9]
			% Axes
			\draw[->] (0.5,0) -- (10,0);
			\draw[->] (1,-0.5) -- (1,4);
			
			% x-axis ticks and labels
			\draw (2,0.1) -- (2,-0.1) node[below] {$s_1$};
			\draw (8,0.1) -- (8,-0.1) node[below] {$s_2$};
			
			\draw[thick, blue, smooth]
			plot coordinates {
				(2,1.5)
				(3,2.5)
				(5,0.8)
				(7.5,3)
				(8,2.5)
			}
			node[right, above, blue,xshift=20] {$x^\star + \eta\xi$};
			
			\draw[thick, black, smooth]
			plot coordinates {
				(2,1.5)
				(3,2)
				(5,1.8)
				(7,2)
				(8,2.5)
			}
			node[right, below, black] {$x^\star$};
			
		\end{tikzpicture}
		
		as $\eta\to0$
		
		$(x^\star+\eta\xi)$ and $x^\star$ differentiable both in magnitude and in derivative.
		
		\hspace*{1cm}$\Rightarrow$ ``$(x^\star+\eta\xi)$ is inside the weak ball of $x^\star$''
		
		\hspace*{0.3cm}EE $\Rightarrow$ detect weak minimizers
		\item To solve ODE we need Boundary conditions (BC).
		
		BC come from the admissible set $x(s_1)=x_1,\;x(s_2)=x_2$
		
		\hspace*{1cm}$\to$ $2n$ equations for a 2$^{nd}$ order ODE in $n$ unknowns.
		
		Two Point Boundary Value Problem (TPBVP)
		\item There exists a reformulation of EE:
		$$ p(s) \coloneqq l_{\dot{x}}(s, x, \dot{x}) \quad \text{Momentum associated with a given } x. $$
		$$ H(s, x, \dot{x}, p) \coloneqq - l(s, x, \dot{x}) + \dot{x}^T p. \quad \text{Hamiltonian.} $$
		
		EE can be rewritten as:
		$$ \dot{x} = H_p(s, x, \dot{x}, p). $$
		$$ \dot{p} = -H_x(s, x, \dot{x}, p). $$
		Canonical equation and $x, p$ canonical Variables.
		
		An immediate benefit of this reformulation is that we easily see the following special cases.
		
		[A] $l(x, \dot{x})$. $l$ does not depend on $s$.
		$$ \frac{d}{ds} H = - l_x^T \dot{x} - l_{\dot{x}}^T \ddot{x} + \dot{x}^T \dot{p} + \ddot{x}^T p = \dot{x}^T \underbracket{\left(\underbrace{\frac{dl_{\dot{x}}}{ds}}_{\dot{p}} - l_x\right)}_{=0\text{ because of EE}} = 0 $$
		$$ H = \text{const.} \coloneq c_1 \quad \text{on stationary solutions.} $$
		
		[B] $l(s, \dot{x})$ no dependence on $x$.
		$$ \frac{d}{ds} p = \dot{p} = 0. $$
		$$ p = c_2. $$
	\end{enumerate}
	
	\begin{lemmabox}[Second-order necessary conditions for $(CV-P_1)$]
		Assume $l \in \mathcal{C}^2$.
		
		If $x^\star$ is a weak minimizer of $[CV-P_1]$, then
		\begin{enumerate}
			\item $x^\star$ satisfies EE
			\item $\nabla_{\dot{x}\dot{x}}^2 l(s, x^\star, \dot{x}^\star) \succeq 0, \quad \forall s \in [s_1, s_2]$. \\
			Legendre condition
		\end{enumerate}
	\end{lemmabox}
	\begin{lemmabox}[First-order sufficient condition for global minimizers]
		Assume that $l(s, x, \dot{x})$ is \underline{jointly convex} in $x$ and $\dot{x}$. \\
		If $x^\star \in \mathcal{D}$ satisfies the EE, then $x^\star$ is a global minimizer.
	\end{lemmabox}
	
	\textbf{Free end-point problems}
	$$
	\mathcal{D} = \{(x, s_2) \in \mathcal{C}^1\left([s_1, \infty) \times (s_1, \infty)\right) \mid x(s_1) = x_1, x(s_2) \text{ free}, s_2 \text{ free} \}
	$$
	First-variation also suitably redefined:
	$$ \delta J(x, s_2; \xi, \sigma) := \lim_{\eta \to 0}\; \frac{J(x+\eta\xi, s_2+\eta\sigma) - J(x, s_2)}{\eta} $$
	$$ = \frac{\partial}{\partial\eta} J(x+\eta\xi, s_2+\eta\sigma) \Big|_{\eta=0} $$
	
	\begin{center}
		\begin{tikzpicture}[>=Stealth, thick, font=\small]
			% Achsen
			\draw[->] (-0.5,0) -- (6,0); % s-Achse
			\draw[->] (0,-0.5) -- (0,3); % x-Achse
			
			% Ticks und Beschriftungen auf der s-Achse
			\draw (1.5, 0.15) -- (1.5, -0.15) node[below] {$s_1$};
			\draw (4.0, 0.15) -- (4.0, -0.15) node[below] {$s_2^*$};
			\draw (5.3, 0.15) -- (5.3, -0.15) node[below] {$s_2^* + \eta\sigma$};
			
			% Kurven
			% Optimale Trajektorie x*
			\draw[blue, thick] (1.5, 1.0) to[out=35, in=175] node[pos=0.5, above=2pt] {$x^*$} (4.0, 2.2);
			
			% Gestörte Trajektorie
			\draw[red, thick] (1.5, 1.0) to[out=-10, in=190] node[pos=0.6, below=5pt] {$x^* + \eta\sigma$} (5.3, 1.2);
			
		\end{tikzpicture}
	\end{center}
	
	[CV-P2] \quad $\underset{x(\cdot), s_2}{\min} \;\varphi(s_2, x(s_2)) + \int_{s_1}^{s_2} l(s, x(s), \dot{x}(s)) \, ds$
	$$ \text{s.t.} \quad x \in \{ y \in \mathcal{C}^1([s_1, s_2]) \mid y(s_1) = x_1 \},\quad s_2 \in (s_1, \infty) $$
	
	\begin{lemmabox}[First order necessary conditions for local minimizers of $(CV-P2)$]
		Suppose $(x^\star, s_2^\star)$ is a weak minimizer of $[CV-P2]$, then
		\begin{enumerate}
			\item $x^\star$ solves EE on $[s_1, s_2^\star]$
			\item The transversal conditions
			\begin{itemize}
				\item[A)] $[l_{\dot{x}} + \varphi_x] \big|_{x=x^\star, s=s_2^\star} = 0$
				\item[B)] $[l - \dot{x}^T l_{\dot{x}} + \varphi_s] \big|_{x=x^\star, s=s_2^\star} = 0$
			\end{itemize}
		\end{enumerate}
	\end{lemmabox}
	The transversal conditions ``replace'' the BC at $s_2$ because in $[CV-P2]$ we have none.
	
	\begin{itemize}
		\item 2A is a vector equation with $n$ components $\to$ it replaces ``$x(s_2) = x_2$''
		\item 2B is a scalar equation $\to$ it provides an equation to find $s_2$
	\end{itemize}
	
	\textbf{``Partially'' free end-point problems}
	\begin{enumerate}
		\item[Case 1] $s_2$ free, $x(s_2) = x_2$ given
		
		Same as Lemma 2.6, but we only use 2B
		
		(2A not needed bc the BC on $x(s_2)$ is given)
		\item[Case 2] $s_2$ fixed, $x(s_2)$ free
		
		Same as Lemma 2.6, but we only use 2A
		\item[Case 3] $s_2, x(s_2)$ are free but related through $\psi: \mathbb{R} \to \mathbb{R}^n, \quad x(s_2) = \psi(s_2)$
		
		We do not need 2A $\to$ it is replaced by $x(s_2) = \psi(s_2)$.
		
		We need an extension to 2B.
		$$ 2B': \left[ l + l_{\dot{x}}^T (\dot{\psi} - \dot{x}) + \varphi_s + \varphi_x^T \dot{\psi} \right]_{x=x^\star, s=s_2^\star} = 0 $$
		
		Note $\psi(s_2) = x_2$
		
		When $\dot{\psi} = 0$, 2B' collapses to 2B.
		
		% Common coordinates
		\newcommand{\sone}{0.8}
		\newcommand{\stwo}{2.4}
		
		\begin{tikzpicture}[scale=1,>=stealth]
			
			% === FIRST GRAPH ===
			\begin{scope}[xshift=0cm]
				% axes
				\draw[->] (0,0) -- (3.2,0);
				\draw[->] (0,0) -- (0,2.6);
				% ticks
				\coordinate (s1) at (\sone,0);
				\coordinate (s2) at (\stwo,0);
				\draw (s1) -- ++(0,-0.06) node[below] {$s_1$};
				\draw (s2) -- ++(0,-0.06) node[below] {$s_2$};
				% dashed horizontal line
				\draw[dashed] (0,2) -- (3.2,2);
				
				% start point (common for all)
				\coordinate (start) at (\sone,0.6);
				
				% upper and lower smooth loops
				\draw[blue,thick,smooth]
				plot coordinates {(\sone,0.6) (1.2,1.3) (1.8,2)};
				\draw[red,thick,smooth]
				plot coordinates {(\sone,0.6) (1.5,0.8) (2.2,1.2) (3,2)};
				
				% label
				\node[below=15pt] at (1.6,0) {\small Case 1};
			\end{scope}
			
			% === SECOND GRAPH ===
			\begin{scope}[xshift=5cm]
				% axes
				\draw[->] (0,0) -- (3.2,0);
				\draw[->] (0,0) -- (0,2.6);
				% ticks
				\coordinate (s1) at (\sone,0);
				\coordinate (s2) at (\stwo,0);
				\draw (s1) -- ++(0,-0.06) node[below] {$s_1$};
				\draw (s2) -- ++(0,-0.06) node[below] {$s_2$};
				% dashed vertical line
				\draw[dashed] (\stwo,0) -- ++(0,2.2);
				
				% same start
				\coordinate (start) at (\sone,0.6);
				
				% top and bottom smooth increasing curves
				\draw[red,thick,smooth]
				plot coordinates {(\sone,0.6) (1.5,1.8) (\stwo,2.0)};
				\draw[blue,thick,smooth]
				plot coordinates {(\sone,0.6) (1.3,0.7) (1.9,0.9) (2.2,1.0) (\stwo,1.05)};
				
				% mark endpoints
				\fill (\stwo,2.0) circle (0.8pt);
				\fill (\stwo,1.05) circle (0.8pt);
				
				% label
				\node[below=15pt] at (1.6,0) {\small Case 2};
			\end{scope}
			
			% === THIRD GRAPH (with two blue curves + psi in red) ===
			\begin{scope}[xshift=10cm]
				% parameters
				\def\sone{0.8}
				\def\stwo{2.3}
				
				% axes
				\draw[->] (0,0) -- (3.2,0);
				\draw[->] (0,0) -- (0,2.6);
				
				% ticks and labels
				\coordinate (s1) at (\sone,0);
				\coordinate (s2) at (\stwo,0);
				\draw (s1) -- ++(0,-0.06) node[below] {$s_1$};
				\draw (s2) -- ++(0,-0.06) node[below] {$s_2$};
				
				% --- blue curves (upper and lower arcs) ---
				\draw[red,thick,smooth]
				plot coordinates {
					(\sone,0.6)
					(1.3,1.8)
					(1.9,2.2)
					(2.72,1.8)
				};
				
				\draw[blue,thick,smooth]
				plot coordinates {
					(\sone,0.6)
					(1.3,1.3)
					(1.6,1.2)
					(1.9,0.97)
				};
				
				% --- golden psi curve ---
				\draw[thick,orange,smooth]
				plot[domain=1.2:3]
				(\x,{0.5 + 0.4*(\x-\sone)^1.8});
				\node[right,orange] at (\stwo,1.2) {$\psi$};
				
				% label
				\node[below=15pt] at (1.6,0) {\small Case 3};
			\end{scope}
			
		\end{tikzpicture}
		
	\end{enumerate}
\end{document}
