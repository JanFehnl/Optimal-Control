\documentclass[12pt,a4paper,oneside]{scrartcl}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{tcolorbox}
\usepackage{microtype}
\usepackage{geometry}
\geometry{left=3cm,right=2.5cm,top=3cm,bottom=3cm}
\usepackage{setspace}
\onehalfspacing

\usepackage{amsmath,amssymb,amsthm,mathtools}
\usepackage{bm}
\usepackage{dsfont}
\usepackage{physics}
\usepackage{nicefrac}
\usepackage{enumitem}
\setlist[itemize]{topsep=0pt, itemsep=2pt}

\usepackage{tikz}
\usetikzlibrary{arrows.meta, positioning}
\usepackage{pgfplots}
\pgfplotsset{compat=1.17}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{etoolbox}
\usepackage{xcolor}
\usepackage{float}
\usepackage{wrapfig}

\KOMAoptions{parskip=half}

\definecolor{indigo}{RGB}{75,0,130}

% Theorem
\newtheorem{lemma}{Lemma}[section]
\newtheorem{definition}{Definition}[section]
\newtheorem{theorem}{Theorem}[section]

\newenvironment{lemmabox}[1][]{
	\refstepcounter{lemma}
	\begin{tcolorbox}[colback=teal!5!white,
		colframe=teal!60!black,
		fonttitle=\bfseries,
		boxrule=0.8pt,
		arc=3pt,
		left=8pt,right=8pt,top=6pt,bottom=6pt,
		title={Lemma \thelemma\if\relax\detokenize{#1}\relax\else\ (#1)\fi}]
		}{
	\end{tcolorbox}
}

\newenvironment{definitionbox}[1][]{
\refstepcounter{definition}
\begin{tcolorbox}[colback=blue!5!white,
	colframe=blue!60!black,
	fonttitle=\bfseries,
	boxrule=0.8pt,
	arc=3pt,
	left=8pt,right=8pt,top=6pt,bottom=6pt,
	title={Definition \thedefinition\if\relax\detokenize{#1}\relax\else\ (#1)\fi}]
	}{
\end{tcolorbox}
}

\newenvironment{theorembox}[1][]{
\refstepcounter{theorem}
\begin{tcolorbox}[colback=indigo!5!white,
	colframe=indigo!70!black,
	fonttitle=\bfseries,
	boxrule=0.8pt,
	arc=3pt,
	left=8pt,right=8pt,top=6pt,bottom=6pt,
	title={Theorem \thetheorem\if\relax\detokenize{#1}\relax\else\ (#1)\fi}]
}{
\end{tcolorbox}
}

% Kopf- und FuÃŸzeile
\usepackage{scrlayer-scrpage}
\clearpairofpagestyles
\ihead{Optimal Control}
\ohead{WS 2025/26}
\cfoot{\pagemark}
\pagestyle{scrheadings}
\setcounter{section}{-1}

\begin{document}
	
	% Titelseite
	\begin{titlepage}
		\centering
		\vspace*{3cm}
		{\Huge\bfseries Optimal Control}\par
		\vspace{1.5cm}
		{\Large Wintersemester 2025/26}\par
		\vspace{0.5cm}
		{\Large Dozent: Prof. Dr. Andrea Iannelli}\par
		\vfill
		{\today}
	\end{titlepage}
	
	\pagenumbering{roman}
	\tableofcontents
	\clearpage
	\pagenumbering{arabic}
	
	\section{Introduction}
	\[
	\dot{x} = f(t,x,u), \quad x(t_0)=x_0, \quad t \in [t_0,t_f]
	\]
	\[
	f : [t_0,t_f]\times\mathbb{R}^{n_x}\times\mathbb{R}^{n_u} \to \mathbb{R}^{n_x}
	\]
	\[
	x = \text{state}, \quad u = \text{input}
	\]
	
	Initial Value Problem (IVP)
	
	Given $x_0, u(\cdot)$ we can compute $x(\cdot)$ \\
	\hspace*{23.5mm}$\rotatebox[origin=c]{90}{$\Rsh$}$ functions of time $\rotatebox[origin=c]{270}{$\Lsh$}$
	
	When is this possible? It depends on $f$.
	
	
	\begin{lemmabox}[Sufficient conditions]
		Existence \& Uniqueness of solutions of ODEs.\\
		Assume that
		\begin{itemize}[]
			\item $f$ is piecewise continuous in $t$ and $u$
			\item $f$ is globally Lipschitz in $x$
			\[
			\exists\, k(t,u)\, \text{ s.t. } \|f(t,x_1,u)-f(t,x_2,u)\|\le k(t,u)\|x_1-x_2\|,\ \forall x_1,x_2 \in \mathbb{R}^{n_x}
			\]
		\end{itemize}
		Then $x(\cdot)$ exists for all $t$ and is unique.
	\end{lemmabox}
	
	\paragraph{Remarks}
	\begin{itemize}[noitemsep]
		\item Lipschitz continuous $\Rightarrow$ continuous, but not the converse
		\item $\sqrt{x}$ is continuous but not Lipschitz, $\dot x = \sqrt{x}$ does not have a unique solution
		\item Continously differentiable $(\mathcal{C}^1)$ $\Rightarrow$ locally Lipschitz continous $\forall x_1,x_2 \in \mathcal{X} \subset \mathbb{R}^{n_x}$
		\item Locally Lipschitz continuous x guarantees existence \& uniqueness for small enough times
	\end{itemize}
	
	In this course we will assume $f \in\mathcal{C}^1$ and implicitely assume that $t_f$ is chosen such that $x(\cdot)$ exists in $[t_0,t_f]$. \\\\
	We do not need to worry about existence \& uniqueness!
	
	\paragraph{Goal in Optimal Control:}
	Design $u$ such that
	\begin{enumerate}
		\item $u(t) \in \underset{\uparrow}{\mathcal{U}(t)}$, $x(t) \in \underset{\uparrow}{\mathcal{X}(t)} \quad \forall t \in [t_0,t_f], \quad \mathcal{X}\subseteq\mathbb{R}^{n_x}, \ \mathcal{U}\subseteq\mathbb{R}^{n_u}$\\
			sets defining constraints on $u \& x$ \\
			\hspace*{10mm}$\Rightarrow$ Admissible input/state trajectories
		\item
			The system behaves optimally according to
			\[
			\underset{\uparrow}{J}(u) = \int_{t_0}^{t_f} \underset{\uparrow}{l}(t,x(t),u(t))\,dt + \underset{\uparrow}{\varphi}(t_f,x(t_f))
			\]
			\hspace*{18mm}Cost function\hspace*{10mm}running cost\hspace*{10mm}terminal cost\\
			\hspace*{10mm}$\Rightarrow$ optimal behaviour
	\end{enumerate}
	
	Formally, we can state the goal as follows: \\
	Find an admissible input $u^\star$ which causes the dynamics to follow an admissible trajectory $x\star$ which minimizes $J$, that is
	\[
	\int_{t_0}^{t_f} l(t,x^\star(t),u^\star(t))\,dt + \varphi(t_f,x^\star(t_f)) \leq \int_{t_0}^{t_f} l(t,x(t),u(t))\,dt + \varphi(t_f,x(t_f))
	\]
	\hspace*{100mm}$\forall \text{ admissible } x,u$
	
	\paragraph{Examples of cost functions}
	\begin{enumerate}[label=\arabic*)]
		\item Minimum-time problem\\
		Goal: transfer the system from $x_0$ to a set $\mathcal{S}$ in the minimum time
		\[
		J = t_f-t_0 = \int_{t_0}^{t_f}dt \qquad (l=1, \varphi=0)
		\]
		\[
		x(t_f) \in \mathcal{S}
		\]
		Note: $t_f$ is also a decision variable! The unknowns are $(u,t_f)$.
		
		\item Minimum control-effort problem
		\[
		J = \int_{t_0}^{t_f} \|u(t)\|^2 \, dt
		\]
		\[
		x(t_f) \in \mathcal{S}
		\]
		
		\item Tracking problem
		\[
		J = \int_{t_0}^{t_f} (x(t)-r(t))^T Q (x(t)-r(t))dt
		\]
		$Q > 0$ (positive definit matrix: symmetric \& all eigenvalues positive) \\
		$r(t)$ given signal
	\end{enumerate}
	
	\section{Nonlinear Programming}
	
	Nonlinear Programs (NLP) are general \underline{finite-dimensional} optimization problems:
	\[
	\underset{x}{\min} f(x)
	\]
	\[
	\text{s.t. } g(x)\leq0, \quad h(x)=0
	\]
	$f: \mathbb{R}^n \to \mathbb{R}$, objective function\\
	$g: \mathbb{R}^n \to \mathbb{R}^{n_g}$, inequality constraints\\
	$h: \mathbb{R}^n \to \mathbb{R}^{n_h}$, equality constraints\\
	Feasible set:
	\[
	D = \{x\in\mathbb{R}^n \mid g(x)\leq0,\ h(x)=0\}
	\]
	$\bar{x}\in D$ feasible point
	
	\begin{definitionbox}[Global, local Minimizers]
			$x^\star\in \mathcal{D}$ \underline{Global Minimizer} of the NLP if
			\[
			f(x^\star)\le f(x)\quad \forall x\in \mathcal{D}
			\]
			$f(x^\star)$ is the \underline{Global Minimum} (or Minimum)\\
			Nomenclature: $x^\star$ is also called (optimal) solution, $F(x^\star)$ is optimal value\\
			$x^\star$ is a strict global minimizer if $f(x^\star)<f(x)\quad \forall x \in\mathcal{D}$\\
			$x^\star\in \mathcal{D}$ \underline{Local Minimizer} if
			\[
			\exists \varepsilon>0,\text{ s.t.}\ f(x^\star)\le f(x)\quad \forall x\in B_\varepsilon(x^\star)\cap \mathcal{D}
			\]
			\[
			B_\varepsilon(x) := \{ y \mid \|x-y\|\le\varepsilon\} \qquad \|\cdot\|:\mathbb{R}^n\to\mathbb{R}_{\geq 0}\text{ any norm in }\mathbb{R}^n
			\]
			Strict local Minimizer if inequality holds strictly\\
			Global min $\begin{tikzpicture}[baseline={(0,-0.25ex)}]
				% oberer Pfeil: nach rechts
				\draw[->] (0,0.2) -- (0.5,0.2);
				% unterer Pfeil: nach links
				\draw[<-] (0,0.0) -- (0.5,0.0);
				% Durchstreichung des unteren Pfeils
				\draw[line width=0.4pt] (0.1,-0.1) -- (0.4, 0.1);
			\end{tikzpicture}$ local min
	\end{definitionbox}
	
	Solving an NLP boils down to finding global or local minimizers. \\
	Does a solution always exist? No.
	
	
	
	
	\begin{definitionbox}[infimum]
		Given $\mathcal{S} \subseteq \mathbb{R}$, $\inf(\mathcal{S})$ is the greatest lower bound of $\mathcal{S}$:
		\begin{itemize}
			\item $z \geq \inf(\mathcal{S}), \quad \forall z \in \mathcal{S}\quad$ (lower bound)
			\item $\forall \bar{\alpha} > \inf(\mathcal{S}) \quad \exists z \in \mathcal{S}$ s. t. $ \bar{\alpha} > z\quad$ (greatest bound)
		\end{itemize}
	\end{definitionbox}
	
	\paragraph{Example}
		$\mathcal{S} = [-1,1], \, -50 = \inf(\mathcal{S})?\quad\to\quad$ No, $\inf(\mathcal{S}) = -1$	
	\begin{itemize}
		\item Analogous: $\sup(\mathcal{S})$ is smallest upper bound.  
		\item $\inf$ and $\sup$ always exist if $\mathcal{S} \neq \emptyset$
		\item $\inf(\mathcal{S})$ does not have to be an element of $\mathcal{S}$
		\item If $\mathcal{S}$ unbounded from below $\to$ $\inf(\mathcal{S})=-\infty$
		\item $\inf([a,b]) = \inf((a,b]) = a$
	\end{itemize}
	
	Connections with NLP?
	\[
	f: \mathcal{D} \to \mathbb{R}
	\]
	\[
	\inf(\underset{\mathcal{S}}{\underbrace{f(x)\,|\,x\in\mathcal{D}}}):=\bar{f}=\underset{x\in\mathcal{D}}{\inf}f(x) \quad\text{(similar to NLP)}
	\]
	
	Whenever NLP has solution, then NLP ist equivalent to this, but $\nexists x^\star \in \mathcal{D}$ s. t. $f(x^\star) = \bar{f}\quad\to\quad$ infimum exists, but not minimum
	
	\paragraph{Examples}
		$f(x) = e^{-x}, \quad \mathcal{D} = [0,\infty), \quad \inf(\mathcal{S}) = 0$ \\
		$f(x) = x, \quad \mathcal{D} =\mathbb{R}, \quad \inf(\mathcal{S}) = - \infty$, min doesn't exist! 
	\begin{center}
		\begin{tikzpicture}
			\begin{axis}[
				axis lines = left,
				grid = both,
				xlabel = $x$,
				ylabel = $f(x)$,
				xmin = 0, xmax = 5,
				ymin = 0, ymax = 1.2,
				width = 10cm,
				height = 5cm,
				every axis plot/.style={very thick, green!60!black},
				]
				
				\addplot[domain=0:5, samples=200]{exp(-x)} node[pos=0.15, above right] {$f$};
				\node at (axis cs:0,1) [anchor=east, green!50!black] {$f$};
				
			\end{axis}
		\end{tikzpicture}
	\end{center}
	When does the infimum coincide with the minimum?
	
	\begin{theorembox}[Extreme value problem) (Weierstrass Theorem]
		$f: D \to \mathbb{R},\,\mathcal{D} \subseteq \mathbb{R}^n$\\
		If:
		\begin{itemize}
			\item $f \in \mathcal{C}$ on $\mathcal{D}$
			\item $\mathcal{D}$ is compact
			\item $\mathcal{D} \neq \emptyset$
		\end{itemize}
		Then $f$ attains a minimum on $\mathcal{D}$.
	\end{theorembox}
	
	\begin{definitionbox}[Continuous function]
		$f: \mathcal{D} \to \mathbb{R}$ is continuous at $x\in\mathcal{D}$ if
		\[
			\forall \epsilon > 0 \exists \delta > 0 \text{ s. t. } \norm{x-x'} < \delta \quad \Rightarrow \quad \norm{f(x)-f(x')} < \epsilon
		\]
		If $f$ is continuous $\forall x \in \mathcal{D}$ then $f$ is continuous on $\mathcal{D}$ $\to$ $f \in \mathcal{C}$
	\end{definitionbox}
	
	Implication for NLP: If $f$ is $\mathcal{C}$ on $\mathcal{D}$ and $\mathcal{D}$ is compact and non-empty then [NLP] has a solution!
	
	\begin{itemize}
		\item $\mathcal{D} \subseteq \mathbb{R}^n$: in finite-dimensional spaces: compact = closed and bounded\\
		Not compact:
		\begin{itemize}
			\item $(a,b]\quad$ (not closed)
			\item $(-\infty,b]\quad$ (unbounded)
		\end{itemize}
		Compact set:
		\begin{itemize}
			\item $[a,b]\quad-\infty<a<b<\infty$
		\end{itemize}
	
	\paragraph{Warning:}
	$\mathcal{D}$ infinite dimensional (e.\,g. function space) then\\
	\hspace*{10mm}compact $\begin{tikzpicture}[baseline={(0,-0.25ex)}]
		% oberer Pfeil: nach rechts
		\draw[->] (0,0.2) -- (0.5,0.2);
		% unterer Pfeil: nach links
		\draw[<-] (0,0.0) -- (0.5,0.0);
		% Durchstreichung des unteren Pfeils
		\draw[line width=0.4pt] (0.1,-0.1) -- (0.4, 0.1);
	\end{tikzpicture}$ bounded and closed
	\begin{center}
		\begin{tikzpicture}
			\begin{axis}[
				axis lines = middle,
				xlabel={$x$},
				ylabel={$f(x)$},
				xmin=-0.7, xmax=0.7,
				ymin=-0.0, ymax=0.45,
				width=6cm,
				height=4cm,
				grid=both,
				every axis plot/.style={very thick, green!60!black},
				]
				
				% Funktion
				\addplot[domain=-0.7:0.7, samples=200]{x^2};
				
				% Minimum markieren
				\addplot[only marks, mark=* , mark size=2.5pt, red] coordinates {(0,0)};
				\node[red, above right] at (axis cs:0,0) {$x^\star$};
				
			\end{axis}
		\end{tikzpicture}
	\end{center}
	Theorem 1.1 is restrictive e.\,g. $f(x) = x^2, \mathcal{D} = (-\infty,\infty)$ has unique minimum
	\item Notation convention: Technically it is ``wrong'' to write
	\[
		\min\underset{x\in\mathcal{D}}{f(x)}
	\]
	more compact is:
	\[
		\text{minimize}\,\underset{x\in\mathcal{D}}{f(x)} \quad \text{or} \quad \underset{x\in\mathcal{D}}{\inf}f(x)
	\]
	\end{itemize}
	
	Goal of the Chapter: characterize necessary and sufficient conditions for $x^\star$ to be global minimizer of NLP.
	
	\subsubsection*{Convexity}
	
	\begin{definitionbox}[Convex sets \& functions]
		\begin{itemize}
			\item A set $C \subseteq \mathbb{R}^n$ is \underline{convex} (cvx) if $\forall x, y \in C$
			\[
			\{ z \mid z = \lambda x + (1-\lambda)\lambda, \, \lambda \in [0,1]\}\subseteq C
			\]
			\begin{center}
				\begin{tikzpicture}[>=Latex]
				
				% Ellipse
				\draw[blue, line width=1pt] (0,0) ellipse [x radius=3.2cm, y radius=1.6cm];
				
				% Punkte
				\coordinate (X) at (-1.25,0.20);
				\coordinate (Y) at ( 1.10,-0.10);
				
				% Verbindungssegment
				\draw[gray!70, line width=0.8pt] (X) -- (Y);
				
				% Markierungen + Labels
				\draw[red!75!black, fill=red!75!black] (X) circle (2.1pt)
				node[above left=1pt, text=red!75!black] {$x$};
				\draw[red!75!black, fill=red!75!black] (Y) circle (2.1pt)
				node[below right=1pt, text=red!75!black] {$y$};
				% Buchstabe C in der Ellipse
				\node[blue] at (-0.25,-0.5) {$C$};
				\end{tikzpicture}
			\end{center}
			
			\item Given a cvx set $C$, a function $f: C \to \mathbb{R}$ is cvx if
			\[
			\textcolor{green!50!black}{f(\lambda x + (1 - \lambda)y)} \leq \textcolor{ purple!60!white}{\lambda f(x) + (1 - \lambda) f(y)}, \quad\forall x,y \in C, \quad\lambda \in (0,1)
			\]
			\begin{center}
				\begin{tikzpicture}[line cap=round, line join=round]
					
					% Parabel: y = a*x^2 + c  (nach oben geÃ¶ffnet)
					\pgfmathsetmacro\a{0.35}
					\pgfmathsetmacro\c{-1.8}
					
					% Punkte auf der Parabel (x-Koordinaten, xA < xB)
					\pgfmathsetmacro\xA{-2.0}
					\pgfmathsetmacro\xB{ 2.4}
					\pgfmathsetmacro\yA{\a*\xA*\xA + \c}
					\pgfmathsetmacro\yB{\a*\xB*\xB + \c}
					
					% Gerade durch die beiden Punkte
					\pgfmathsetmacro\m{(\yB-\yA)/(\xB-\xA)}
					\pgfmathsetmacro\b{\yA - \m*\xA}
					
					% --- Parabel zuerst komplett SCHWARZ ---
					\draw[very thick, black, domain=-3.0:3.0, samples=250]
					plot ({\x},{\a*\x*\x + \c});
					
					% --- Dann den Abschnitt zwischen xA und xB in GRÃœN Ã¼berzeichnen ---
					% (fÃ¼r nach oben geÃ¶ffnete Parabel liegt dieser Abschnitt unterhalb der Geraden)
					\draw[very thick, green!50!black, domain=\xA:\xB, samples=200]
					plot ({\x},{\a*\x*\x + \c});
					
					% Lilafarbene Sehne
					\draw[line width=2pt, purple!60!white] (\xA,\yA) -- (\xB,\yB);
					
					% Rote Punkte + Labels
					\fill[red!70!black] (\xA,\yA) circle (2.2pt)
					node[left=3pt, text=red!70!black] {$x$};
					\fill[red!70!black] (\xB,\yB) circle (2.2pt)
					node[right=3pt, text=red!70!black] {$y$};
					
				\end{tikzpicture}				
			\end{center}
			\item f is strictly cvx if the inequality holds strictly.
		\end{itemize}
		
	\end{definitionbox}
	
	\paragraph{Remarks}
	\begin{itemize}
		\item The definition extends to vector functions $f:C\to\mathbb{R}^n$ for convex $f_i$
		\item $f: C_1 \cross C_2 \to \mathbb{R}$\\
		$f(x,y)$ is jointly cvx, in $x,y$ if $z:=\begin{bmatrix}x\\y\end{bmatrix},\,f(z)$ is in cvx in $z$.
	\end{itemize}
		
	
	
	\paragraph{Example}
		$f(x,y) = x^2+y^2,\,z=\begin{bmatrix}x\\y\end{bmatrix}\to f(z) = z_1^2+z_2^2$
	
	\begin{definitionbox}
		An NLP is a \underline{convex program} if
		\begin{itemize}
			\item $f$ is convex function,
			\item $\mathcal{D}$ is convex set.
		\end{itemize}
	\end{definitionbox}
	
	\begin{lemmabox}
		Let $x^\star$ be a local minimizer of cvx program.  
		Then $x^\star$ is also global minimizer.
	\end{lemmabox}
	
	\textbf{Proof:} try as an erxercise\\
	
	Minimizers of convex NLP form a convex set.\\
	This set might be empty (Convex NLPs not guaranteed to have solution).\\
	\underline{However}: Unique solution for \underline{strictly} convex NLPs, if a solution exists.
	
	\begin{lemmabox}[First/Second order conditions for convexity]
		\begin{enumerate}
			\item $f:C\to\mathbb{R}$ continuously differentiable on $C$. Then $f$ is cvx iff
			\[
				f(y)\geq f(x)+\nabla f(x)^T(y-x), \quad\forall x,y\in C
			\]
			\[
				(\nabla f)_i = \frac{\partial f}{\partial x_i} \text{ is gradient (sometimes $f_{x_i}$)}
			\]
			\item $f$ twice differentiable on $C$, then $f$ convex iff
			\[
				\nabla_{xx}^2 f(x) \geq 0 \quad \forall x \in C
			\]
			\[
				(\nabla_{xx}^2)_{i,j} = \frac{\partial^2 f}{\partial x_i \partial x_j} \quad \text{(Hessian)}
			\]
		\end{enumerate}
	\end{lemmabox}
	
	\begin{itemize}
		\item $A\geq0$ means that: $A=A^T$ and pos semi-definite, i.\,e. all eigenvalues non-negative
		\item $f$ strictly cvx if $\nabla_{xx}^2 f(x) > 0 \quad \forall x \in C$ with $A>0$ meaning pos definite and symmetric
		\item Interpretation: Curvature of function should be non-negative/positive
		\item For exercises to check convexity, the second condition is generelly useful. First condition is useful for proofs.
	\end{itemize}
	
	For $\mathcal{D} = \{ x \mid g(x) \leq 0, \, h(x) = 0 \}$ the following holds: If
	
	\begin{minipage}{0.65\textwidth}
		\begin{itemize}
			\item $g$ are convex functions,
			\item $h$ are affine functions (i.e. $h(x) = 0 \Leftrightarrow Ax=b$),
		\end{itemize}
	\end{minipage}
	\begin{minipage}{0.15\textwidth}
		$\left.\vphantom{\begin{array}{l}g\\h\end{array}}\right\}$ sufficient
	\end{minipage}
	
	
	then $\mathcal{D}$ is a convex set.
	
	\paragraph{Example} $a,b\in\mathbb{R}$
	
	\begin{table}[H]
		\centering
		\begin{tabular}{c c c}
			$\underset{x}{\min}f$ & & $\underset{x}{\min}f$\\
			s.t. $x^3-1\leq0$ & $\qquad\leftrightarrow\qquad$ & s.t. $x-1\leq0$\\
			$(ax+b)^2=0$ & & $ax+b=0$\\
			non-convex & & convex\\
			non-affine & & affine
		\end{tabular}
	\end{table}
	
	\underline{Moral to recognize convexity of NLP:}
	\begin{enumerate}
		\item Use definition of cvx NLP, cvx $f$, convex $\mathcal{D}$
		\item If $\mathcal{D}$ written as equality/inequality-constraints, check $g$ convex/$h$ affine.\\
		If not, check further whether the feasible set is cvx or not (e.g. can be written equivalently with cvx $g$/affine $h$).
	\end{enumerate}
	
	\subsection{Unconstrained Problems}
	\[
		\mathcal{D} = \mathbb{R}^n
	\]
	Assume throughout that $f \in \mathcal{C}^1$ (continuously differentiable).
	
	\begin{definitionbox}[Descent Direction]
		$d \in \mathbb{R}^n$ is a \underline{descent direction} for $f$ at $\bar{x} \in \mathbb{R}^n$ if
		\[
		\exists \delta > 0 \quad \text{s.t.} \quad f(\bar{x} + \lambda d) < f(\bar{x}) \quad \forall \lambda \in (0, \delta).
		\]
		$F(\bar{x})$: \underline{Cone of decent directions}\\
		Set of all descent directions of $f$ at $\bar{x}$
	\end{definitionbox}
	
	A set $K \subseteq \mathbb{R}^n$ is a cone if it contains the full ray through any point in the set.
	
	\[
		K \text{ cone if }\forall x\in K \text{ and } \rho \geq 0, \quad \rho x \in K
	\]
	\begin{center}
		\begin{tikzpicture}[line cap=round,line join=round,>=stealth,thick,scale=1]
			
			% Achsen
			\draw[->] (-0.2,0) -- (5,0);
			\draw[->] (0,-0.2) -- (0,4);
			
			% Parameter
			\pgfmathsetmacro{\mA}{0.3}   % untere schwarze Linie
			\pgfmathsetmacro{\mB}{0.8}   % obere schwarze Linie
			\pgfmathsetmacro{\mR}{0.6}   % rote Mittellinie
			\pgfmathsetmacro{\L}{4.5}    % LÃ¤nge
			
			% Koordinaten
			\coordinate (A) at (\L,\mA*\L);
			\coordinate (B) at (\L,\mB*\L);
			\coordinate (R) at (\L,\mR*\L);
			
			% Gelber Bereich zwischen den schwarzen Linien
			\path[fill=yellow!50,opacity=0.6] (0,0) -- (A) -- (B) -- cycle;
			
			% Schwarze Begrenzungslinien
			\draw[thick] (0,0) -- (A);
			\draw[thick] (0,0) -- (B);
			
			% Rote Mittellinie
			\draw[red,very thick] (0,0) -- (R);
			
			% Punkt x auf der roten Linie
			\coordinate (P) at (2.5, {2.5*\mR});
			\fill[black] (P) circle (2pt);
			\node[below right,blue,scale=1.3] at (P) {$x$};
			
		\end{tikzpicture}
		
	\end{center}	
	This is a geometric characterization of descent direction. It gives us a geometric condition for $x^\star$ to be a local minimizer.
	
	\begin{lemmabox}[Geometric Condition for local minimum]
		$x^\star$ is a local minimizer iff
		\[
		\mathcal{F}(x^\star) = \emptyset.
		\]
	\end{lemmabox}
	
	We want an algebraic condition to be able to compute or look for $x^\star$.
	
	\begin{lemmabox}[Algebraic first-order characterization of $\mathcal{F}$]
		If $\nabla f(\bar{x}) \neq 0$, then
		\[
		\mathcal{F}_0(\bar{x}) = \{ d \mid \nabla f(\bar{x})^T d < 0 \} = \mathcal{\mathcal{F}}(\bar{x}).
		\]
		Otherwise
		\[
		\mathcal{F}_0(\bar{x}) \subseteq \mathcal{F}(\bar{x}).
		\]
	\end{lemmabox}
	
	\textbf{Proof:} try Taylor-series expansion of $f$ at $\bar{x}$
	
	Graphical interpretation:
	
	
	
	$\nabla f$ forms angles greater or equal than $90^\circ$ with \underline{all} descent directions.
	
	\begin{lemmabox}[First-order necessary condition for local minimum]
		If $x^\star$ is a local minimizer, then
		\[
		\underset{\text{``stationary point''}}{\underbrace{\nabla f(x^\star) = 0}}.
		\]
	\end{lemmabox}
	
	\textbf{Proof:} Contradiction\\
	If $\nabla f(x^\star) \neq 0$, then $d = -\nabla f(x^*) \neq 0$. Therefore there exists a descent direction $d\in\mathcal{F}(x^\star)$ by Lemma 1.4. Thus $\exists \delta > 0$ s.t. $f(x^\star+\lambda d)<f(x^\star) \quad \forall\lambda\in (0,\delta).$\\
	This is a contradiction with the fact, that $x^\star$ is a minimizer.\hfill\qedsymbol{}
	
	Why only necessary?\\
	It can't be a sufficient condition because in case where $\nabla f(x^\star) = 0$ we cannot use Lemma 1.4, e.g. $f_1(x) = -x^2,\,f_2(x) = x^3,\,\nabla f_1(0) = \nabla f_2(0) = 0.$
	
	\begin{lemmabox}[second order necessary condition]
		Assume f is twice continuously differentiable $f \in \mathcal{C}^2$
		\[
			x^\star\text{ local minimizer }\Rightarrow \nabla_{xx}^2 f(x^\star) \geq 0
		\]
	\end{lemmabox}
	
	\textbf{Note:} the condition on the Hessian of f can be interpreted as a local convexity property (around $x^\star)$.
	
	\textbf{Proof:} $2^{nd}$ order Taylor expansion around $x^\star$ in direction $d \in \mathbb{R}^n$:
	\[
		f(x^\star + \lambda d) = f(x^\star) + \lambda\nabla f(x^\star)^Td + \frac{\lambda^2}{2}d^T \nabla_{xx}^2 f(x^\star) d + \lambda^2\norm{d}^2\alpha (\lambda d)
	\]
	\[
		(\to\alpha(\cdot)) \text{ is a function that is order $1$ or higher in $\lambda d$})
	\]
	\begin{enumerate}
		\item If $x^\star$ is local minimzer $\Rightarrow\nabla f(x^\star) = 0$
		\item Divide by $\lambda^2$:
		\[
			\frac{f(x^\star+\lambda d)-f(x^\star)}{\lambda^2} = \frac{1}{2} d^T \nabla_{xx}^2 f(x^\star) d + \norm{d}\alpha (\lambda d)
		\]
		\item $\lambda \to 0$ on the right-hand-side the first term dominates
		\[
			\frac{f(x^\star+\lambda d)-f(x^\star)}{\lambda^2} \approx \frac{1}{2} d^T \nabla_{xx}^2 f(x^\star) d
		\]
		\item For $x^\star$ is a local minimizer, the left-hand-side must be $\geq 0$ for any $d\in\mathbb{R}^n$\\$\hspace*{1cm}\Rightarrow\quad d^T \nabla_{xx}^2 f(x^\star) \geq 0 \quad\forall d\quad \Rightarrow\quad \nabla_{xx}^2 f(x^\star)\geq0$\hfill\qedsymbol{}
	\end{enumerate}
	
	Only a necessary condition, because when $\nabla_{xx}^2 f(x^\star)$ is singular, we need to use higher-order information.
	
	Generally it is hard to get (global) sufficient conditions. $\to$ convexity to the rescue!
	
	\begin{lemmabox}[First order N\&S condition for global minimizers]
		Assume f is convex.
		\[
			\exists x^\star \text{ s.t. } \nabla f(x^\star) = 0 \quad\Leftrightarrow\quad x^\star \text{ is a global minimizer}
		\]
		If $f$ is  strictly convex, then the minimizer is unique.
	\end{lemmabox} 	
	
	\textbf{Proof:} $(\nabla f = 0\,\Rightarrow\,\text{global minimum})$
	
	First order condition for convexity:
	\[
		f(y) \geq f(x) + \nabla f(x)^T(y-x),\quad\forall x,y\in\mathbb{R}^n
	\]
	Pick $x = x^\star$:
	\[
		f(y) \geq f(x^\star),\quad\forall y\in\mathbb{R}^n
	\]
	(Other direction holds because of Lemma 1.5)
	
	What if we do not have global convexity?
	
	\begin{lemmabox}[Second order sufficient condition for local minimizer]
		Assume $f \in\mathcal{C}^2$.\\
		If $\nabla f(x^\star) = 0$ and $\nabla_{xx}^2 f(x^\star) > 0\quad\Rightarrow\quad x^\star$ is strict local minimizer.
	\end{lemmabox}
	
	\textbf{Proof:} Taylor expansion (Similar to Lemma 1.6)
	
	\subsection{Constrained Problems}
	
	\[
		\mathcal{D} \subseteq\mathbb{R}^n,\quad\mathcal{D} = \{x \mid g_i(x)\leq0,\,i=1,\dots,n_g\quad h_j(x)=0,\,j=1,\dots,n_h\}
	\]
	
	We assume throughout $g_i,\,h_j$ are all $\mathcal{C}^1$ functions.
	
	\begin{definitionbox}[Tangent vector, tangent cone]
		$p\in\mathbb{R}^n$ is a tangent vector to $\mathcal{D}$ at $\bar{x}\in\mathcal{D}$ if $\exists$ differential curve $\bar{x}(s):[0,\epsilon)\to\mathcal{D}$ with $\epsilon>0$ such that $\bar{x}(0)=\bar{x},\left.\frac{d\bar{x}}{ds}\right|_{s=0}=p$.\\\\
		Tangent cone $\mathcal{T}_\mathcal{D}(\bar{x})$ to $\bar{x}$ is the set of all tangent vectors
		\[
			\mathcal{T}_\mathcal{D}(\bar{x}):=\{p\mid p \text{ tangent vector to $\mathcal{D}$ at $\bar{x}$}\}
		\]
	\end{definitionbox}
	
	Graphical representation:
	
	Set of directions that make us stay feasible (at least infinitesimally)
	
	When it comes to geometric conditions for optimality in constrained problems, we now have 2 sets/2 directions:
	\begin{itemize}
		\item $d\in\mathcal{F}(x)\to$ descent direction: objective improves
		\item $d\in\mathcal{T}_D(\bar{x})\to$ tangent vector: we stay feasible
	\end{itemize}
	
	\begin{lemmabox}[Geometric condition for local minimizer, $\mathcal{D}\subseteq\mathbb{R}^n$]
		$x^\star$ is a local minimizer iff $\mathcal{F}(\bar{x})\cap\mathcal{T}_\mathcal{D}(\bar{x})=\emptyset$
	\end{lemmabox}
	
	It is basically says that ``any improving direction can't be feasible''.
	
	
	
	As in the unconstrained case, we want to turn geometric conditions to algebraic ones.
	
	\begin{lemmabox}[$1$st order Nec. condition - semi-algebraic]
		If $x^\star$ is a local minimizer. Then:
		\begin{enumerate}
			\item $x^\star\in\mathcal{D}$
			\item $\underset{\text{geometric}}{\underline{\forall p\in\mathcal{T}_\mathcal{D}(x^\star)}}$, it holds $\underset{\text{algebraic}}{\underline{p^T\nabla f(x^\star)\geq 0}}$
		\end{enumerate}
	\end{lemmabox}
	
	\textbf{Proof:}
	
	Item 1 $\to$ feasibility
	
	Item 2: Assume there is a $p$ s.t. $p^T\nabla f(x^\star)<0$. Then 
	\[
		\exists\text{ curve }\bar{x}(s)\in\mathcal{D}\text{ s.t. }\left.\frac{df(\bar{x})}{ds}\right|_{s=0} \underset{\rotatebox[origin=c]{90}{$\Rsh$}\text{ chain rule }}{=p^T\nabla f(x^\star})<0
	\]
	which would mean that $p$ is descent direction. Contradicts $x^\star$ local minimizer.
	
	This is almost a translation of Lemma 1.9 because we replaced $\mathcal{F}(x^\star)$ with its \underline{algebraic} form ``$d^T\nabla f(x^\star)<0$''.
	
	To obtain a fully algebraic test, we need a few more concepts.
	
	\begin{definitionbox}[Active constraints, active set, regular points]
		$\bar{x}\in\mathcal{D}$
		\begin{itemize}
			\item $g_i$ is \underline{active} at $\bar{x}$ if $g_i(\bar{x})=0$
			\item $A(\bar{x}) = \{i\mid g_i(\bar{x})=0\}$ \underline{set of active constraints} at $\bar{x}$
			\item $\bar{x}\in\mathcal{D}$ is a \underline{regular point} if $\nabla g_i(\bar{x}),\,i\in\mathcal{A}(\bar{x})$ and $\nabla h_j(\bar{x}),\,j=1,\dots,n_h$ are linearly independent.
		\end{itemize}
	\end{definitionbox}
	
	\begin{lemmabox}[Algebraic first-order characterization of target set]
		If $\bar{x}$ is regular point. Then
		\[
			\mathcal{T}_\mathcal{D}(\bar{x})=\{p\mid\nabla h(\bar{x})^p=0,\,\nabla g_i(\bar{x})^p\leq0,\quad\forall i\in\mathcal{A}(\bar{x})\}\qquad\textcircled{1}
		\]
		where $\nabla h(\bar{x}):=[\nabla h_1(\bar{x}),\dots,\nabla h_{n_k}(\bar{x})]\in\mathbb{R}^{n\cross n_h}$.
		
		\textcircled{1} can be written equivalently as $\mathcal{T}_\mathcal{D}(\bar{x})=\{p\mid\mathcal{A}(\bar{x})p\geq0\}$
	\end{lemmabox}
	
	\[
	A(\bar{x}):=\left[
	\begin{array}{c}
		\nabla h(\bar{x})^T \\[4pt]
		-\nabla h(\bar{x})^T \\[4pt]
		\vdots \\[4pt]
		-\nabla g_i(\bar{x})^T \\[4pt]
		\vdots
	\end{array}
	\right]
	\begin{array}{l}
		\in\mathbb{R}^{(2n_h+\abs{\mathcal{A}(\bar{x})})\cross n} \\[6pt]
		\left.\rule{0pt}{3.0em}\right\} i\in\mathcal{A}(\bar{x})
	\end{array}
	\]
	
	In other words, item 2 of Lemma 1.10 can be written as follows:
	\[
		p\in\mathbb{R}^n:\, \mathcal{A}(x^\star)p\geq0,\,p^T\nabla f(x^\star)<0
	\]
	still not very tractable?
	
	Farkas Lemma to the rescue:
	
	\begin{lemmabox}[Farkas Lemma]
		For any matrix $A\in\mathbb{R}^{m\cross n}$, vector $b\in\mathbb{R}^n$.
		
		Exactly one of the following holds:
		\begin{enumerate}
			\item $\exists y\in\mathbb{R}^m\,y\geq0$, such that $A^Ty=b$
			\item $\exists p\in\mathbb{R}^m$, such that $A^Tp\geq0,\,p^Tb<0$
		\end{enumerate}
	\end{lemmabox}
	
	Take $A\equiv A(x^\star)$ and $b\equiv\nabla f(x^\star)$
	
	If we find $y$ satisfying 1., then 2. can't hold $\Rightarrow$ item 2 of Lemma 1.10 is verified $\Rightarrow$ $x^\star\in\mathcal{D}$ is a local minimizer.
	
	KTK-conditions just follow from imposing
	
	\hspace*{1cm}item 1 of Lemma 1.10 $\to$ $x^\star\in\mathcal{D}$
	
	\hspace*{1cm}item 2 of Lemma 1.10 $\to$ $\exists y\in\mathbb{R}^m,\,y\geq0$ s.t. $\mathcal{A}(x^\star)^Ty=\nabla f(x^\star)$
	
	\textbf{\underline{Conceptual summary}:}\nopagebreak
	
	\begin{tikzpicture}[
		node distance=2.5cm and 5cm,
		box/.style={draw, rectangle, rounded corners, align=center, minimum width=4cm, minimum height=1.2cm},
		arrow/.style={-{Latex[length=3mm]}, thick}
		]
		
		% Nodes
		\node[box] (lemma19) {Lemma 1.9\\ \textit{exact geometric}\\ \textit{characterization of $x^\star$}};
		\node[box, right=of lemma19] (lemma110) {Lemma 1.10\\ \textit{semi-algebraic}\\ \textit{ necessary condition}};
		\node[box, below=of lemma110] (kkt) {KKT conditions\\ \textit{(1$^{st}$ order necessary conditions)}};
		
		% Arrows
		\draw[arrow] (lemma19) -- node[above, sloped, align=center] {algebraic expression\\ for $\mathcal{F}(x^\star)$} (lemma110);
		\draw[arrow] (lemma110) -- node[left, align=left]{- algebraic expression of $\mathcal{T}_\mathcal{D}(x^\star)$\\ - Farkas Lemma} (kkt);
	\end{tikzpicture}
	
	\textbf{\underline{Informal recap}:}
	
	$x^\star$ local minimizer $\Rightarrow$ $x^\star\in\mathcal{D},\quad\forall p\in\mathcal{T}_D(x^\star),\quad p^\mathcal{T}\nabla f(x^\star)\geq0$
	
	\hspace*{8cm}$\Updownarrow$ (if $x^\star$ regular point)
	
	$\exists y=\left[
	\begin{array}{c}
		\hat{\lambda}_1 \\[4pt]
		\hat{\lambda}_2 \\[4pt]
		\vdots \\[4pt]
		-\nu_i \\[4pt]
		\vdots
	\end{array}
	\right]
	\begin{array}{l}
		\in\mathbb{R}^{2n_h+\abs{\mathcal{A}(x^\star)}} \\[15pt]
		\left.\rule{0pt}{3.0em}\right\} i\in\mathcal{A}(\bar{x})
	\end{array}\qquad y\geq0\quad\to A(x^\star)^Ty=\nabla f(x^\star)$
	
	\rule{\textwidth}{1pt}
	
	Let's write down $A^Ty=\nabla f$
	\[
		\nabla h(x^\star)(\hat{\lambda}_1-\hat{\lambda}_2)-\sum_{i\in\mathcal{A}(x^\star)}\nabla g_i(x^\star)\nu_i=\nabla f(x^\star),\quad\hat{\lambda}_1,\hat{\lambda}_2,\nu_i\geq0:\text{ But }(\hat{\lambda}_1-\hat{\lambda}_2)\gtreqless0
	\]
	Equivalently: $\lambda:=-(\hat{\lambda}_1-\hat{\lambda}_2)\in\mathbb{R}^{n_h}$, sign undefined
	\[
		\exists\lambda\in\mathbb{R}^{n_h},\quad\nu\in\mathbb{R}^{n_g},\quad\nu\geq0,\quad\nu_i=0,\quad i\notin\mathcal{A}(x^\star)
	\]
	\[
		\text{s.t. }\nabla f(x^\star)+\nabla h(x^\star)\lambda+\nabla g(x^\star)\nu=0
	\]
	\[
		\text{with }\nabla g(x^\star):=\begin{bmatrix}
			\nabla g_1(x^\star)&\nabla g_2(x^\star)&\cdots&\nabla g_{n_g}(x^\star)
		\end{bmatrix}
	\]
	\[
	\text{and }\nabla h(x^\star):=\begin{bmatrix}
		\nabla h_1(x^\star)&\nabla h_2(x^\star)&\cdots&\nabla h_{n_h}(x^\star)
	\end{bmatrix}
	\]
	
	We are now ready for a fully algebraic characterization.
	
	\begin{definitionbox}[Karash-Kuhn-Tucker (KKT) points]
		A triplet of vectors $(\bar{x},\bar{\lambda},\bar{\nu})\in\mathbb{R}^n\cross\mathbb{R}^{n_h}\cross\mathbb{R}^{n_g}$
		
		\hspace*{2cm}$\bar{x}:\text{ opt. variable},\quad\bar{\lambda}:\text{ multiplier equality constraints},$
		
		\hspace*{2cm}$\bar{\nu}:\text{ multiplier inequality constraints}$
		
		is a KKT point if
		\begin{enumerate}
			\item $\nabla f(\bar{x})+\nabla h(\bar{x})\bar{\lambda}+\nabla g(\bar{x})\bar{\nu}=0$
			\item $g(\bar{x})\leq0$
			\item $h(\bar{x})=0$
			\item $\bar{\nu}\geq0$
			\item $\bar{\nu}^Tg(\bar{x})=0$
		\end{enumerate}
	\end{definitionbox}
	
	\begin{lemmabox}[KKT necessary condition for local minimizer]
		If $x^\star$ is a local minimizer \textbf{and} a regular point.
		
		Then $\exists\lambda^\star,\nu^\star$ s.t. $(x^\star,\lambda^\star,\nu^\star$ is a KKT point)
	\end{lemmabox}
	
	\textbf{Proof:} Corollary of previous discussion
	\begin{enumerate}
		\item $\Longleftrightarrow\exists\lambda\in\mathbb{R}^{n_h},\,\nu\in\mathbb{R}^{n_g}$ s.t. $\nabla f(x^\star)+\nabla h(x^\star)\lambda+\nabla g(x^\star)\nu=0$ It can be written
		
		\hspace*{2em} equivalently as
		\[
			\left.\nabla_x\mathcal{L}(x,\lambda,\nu)\right|_{x=x^\star,\lambda=\lambda^\star,\nu=\nu^\star}=0
		\]
		where $\mathcal{L}(x,\lambda,\nu):=f(x)+\lambda^Th(x)+\nu^Tg(x)$
		\item $\Longleftrightarrow x^\star\in\mathcal{D}$
		\item $\Longleftrightarrow x^\star\in\mathcal{D}$
		\item $\Longleftrightarrow$ non-negativity of ``$y$'' from Farkas Lemma
		\item $\Longleftrightarrow\nu_i=0,\,i\notin\mathcal{A}(x^\star)$
		
		
		\hspace*{2em} $\nu^T g(x^\star) = \sum_i \nu_i g_i(x^\star) = 0\quad$ Complementary slackness
		
		\hspace*{2em} $g_i(x^\star) = 
		\begin{cases}
			=0, & i \in \mathcal{A}(x^\star) \\
			<0, & i \notin \mathcal{A}(x^\star)
		\end{cases}
		\hspace*{0.5em} \text{because } x^* \in \mathcal{D}$
		
		\hspace*{2em} $\nu_i \ge 0, \; \forall i\quad$ because of Farkas' lemma.
		
		Then $\sum_i \nu_i g_i(x^\star)$ automatically sets 
		$\nu_i = 0$ when $i \notin \mathcal{A}(x^\star)$ or $g_i(x^\star) < 0$.\hfill\qedsymbol{}
	\end{enumerate}
	
	Intrestingly, if NLP is convex, KKT conditions are sufficient for global optimality:
	
	\begin{lemmabox}[KKT sufficient conditions fot global minimizer]
		Suppose $f,g_i\quad(i=1,\dots,n_g)$ are convex functions and
		
		\hspace*{2cm}$h_j\quad(j=1,\dots,n_h)$ are affine functions.
		
		If $(x^\star,\lambda^\star,\nu^\star)$ is a KKT point, then $x^\star$ is a local minimizer.
	\end{lemmabox}
	
	\textbf{Proof:} For $(\lambda^\star,\nu^\star)$ KKT points:
	
	\[
		b(x):=\mathcal{L}(x,\lambda^\star,\nu^\star)=f(x)+\sum_{i=1}^{n_g}\nu_i^\star g_i(x)+\sum_{j=1}^{n_h}\lambda_j^\star g_j(x)\hspace*{2cm} \textcircled{$\cross$}
	\]
	$f,g_i,h_j$ are convex functions
	
	Linear combination of cvx functions with non-negative coefficients is a convex function
	
	\hspace*{2cm}$\Rightarrow$ $b(x)$ convex
	
	\begin{enumerate}
		\item $b(x)$ convex
		\item $\nabla b(x^\star)=0$ because of $1.$, $(x^\star,\lambda^\star,\nu^\star)$ is a KKT point $b(x)\geq b(x^\star)\quad\forall x\in\mathbb{R}^n$
		
		\hspace*{5cm}$\Updownarrow$ (if $x^\star$ regular point)
		
		\[
			f(x)-f(x^\star)\geq-\underset{\leq0}{\underbracket{\sum_{i\in\mathcal{A}(x^\star)}\nu_i^\star g_i(x)}}-\underset{=0}{\underbracket{\sum_{j=1}^{n_h}\lambda_i^\star h_i(x)}}\geq0,\qquad x\in\mathcal{D}
		\]
		
		\hspace*{2.8cm}because $g(x)\leq0,\,\nu^\star\geq0$\hspace*{0.8cm}because $h(x)=0$
		
		$\to$ $x^\star$ is a global minimizer.\hfill\qedsymbol{}
	\end{enumerate}
	
	\underline{\textbf{Second-order conditions}}
	
	Similary to the unconstrained case, we can use the Hessian.
	
	\[
		\nabla_{xx}^2\mathcal{L}
	\]
	
	We need to check positive semi-definiteness of the Hessian only along feasible directions:
	
	Precisel, we are interestes in this property along
	
	\[
		\text{Critical Directions}=\{p\mid \underset{\text{feasible directions}}{\underbrace{p\in\mathcal{T}_\mathcal{D}(x^\star)}},\quad\underset{\substack{\text{directions that cannot}\\\text{be excluded based on}\\\text{on first order arguments}}}{\underbrace{\nabla f(x^\star)^Tp=0}}\}
	\]
	
	\[
	\left\{
	\begin{aligned}
		\nabla f(x^*)^T p &< 0 \quad \to \quad p \text{ descent direction: }\substack{\text{already excluded by necessary}\\\text{condition of order 1}},\\[0.3em]
		\nabla f(x^*)^T p &> 0 \quad \to \quad p \text{ ascent direction: ``not harmful''},\\[0.3em]
		\nabla f(x^*)^T p &= 0 \quad \to \quad \text{this is what is ``new'' compared to first order.}
	\end{aligned}
	\right.
	\]
	
	\begin{lemmabox}[Second order necessary condition]
		\begin{itemize}
			\item $f, g, h \in \mathcal{C}^2$ at $x^\star$
			\item $x^\star$ local minimizer and regular point
			\item $(x^\star, \lambda^\star, \nu^\star)$ KKT point (which exists by Lemma 1.13)
		\end{itemize}		
		
		Then 
		\[
			p^T \nabla_{xx}^2 L(x^*, \lambda^*, \nu^*) p \ge 0\quad \text{(curvature non-negative along critical directions)}
		\]

		$\forall p \ne 0$ with
		
		\begin{itemize}
			\item $\nabla h(x^\star)^T p = 0$
			\item $\nabla g_i(x^\star)^T p \le 0 \quad\forall i\in\mathcal{A}(x^\star)$ with $\nu_i^\star = 0\mid p \in \mathcal{T}_\mathcal{D}(x^\star)$
			\item $\nabla g_i(x^\star)^T p = 0 \quad\forall i\in\mathcal{A}(x^\star)$ with $\nu_i^* > 0\mid\nabla f(x^\star)^T p = 0$
		\end{itemize}
	\end{lemmabox}
	
	\begin{lemmabox}[Second order suffiecient conditions for local minimizer]
		If $(x^\star,\lambda^\star,\nu^\star)$ is a KKT point with
		
		\[
			p^T\nabla_{xx}\mathcal{L}(^\star,\lambda^\star,\nu^\star)>0
		\]
		
		for same $p$ as in Lemma 1.15.
		
		Then $x^\star$ is a strict local minimizer.
	\end{lemmabox}
	
	\section{Calculus of Variations}
	
	Goal in OC: Find a function that maximizes a functional (function of function) subject \hspace*{2.3cm}to dynamic constraints
	
	In Chapter 1 we characterized solutions to optimization problems over vectors $(\mathbb{R}^n)$
	
	\[
		\min f(x) \text{ s.t. }x\in\mathcal{D}\subseteq\mathbb{R}^n,\qquad\text{Static problem}
	\]
	
	\begin{itemize}
		\item We should introduce ``time'' or ``stages'' in the problem
		\[
			\min_{x_1,\dots,x_N}\;\sum_{k=1}^Nf(k,x_k,x_{k-1})\qquad\text{N coupled stages}
		\]
		\[
			\text{s.t. }x_k\in\mathcal{D}_k\subseteq\mathbb{R}^n,\quad k=1,\dots,N,\quad x_0\text{ given}
		\]
		equivalent to: (loses structure)
		\[
			z=\begin{bmatrix}
				x_1\\\vdots\\x_N
			\end{bmatrix}\to\min_z\;p(z)\qquad z\in\mathcal{Z},\quad z\in\mathbb{R}^{n\cross N}
		\]
		\item Continuous-time description of dynamics
		
		From $N$ stages to continuous time by taking $\infty$ many stages:
		
		\begin{center}
			\begin{tikzpicture}[scale=1.1, >=Latex]
			
			% --- Linkes Diagramm: diskrete Variablen ---
			\begin{scope}
				% Achsen
				\draw[->] (0,0) -- (4.4,0) node[right] {$i$};
				\draw[->] (0,-0.3) -- (0,3) node[left] {$x$};
				
				% Punkte und horizontale Linien
				\foreach \x/\y in {1/1.3, 2/2.3, 3/2.0, 4/1.7}{
					\draw[thick] (\x,0) -- (\x,\y);
					\fill[blue] (\x,\y) circle (2pt);
					\draw[blue, dashed] (0,\y) -- (\x,\y);
				}
				
				% Achsenbeschriftungen
				\node[below] at (1,0) {$1$};
				\node[below] at (4,0) {$N$};
				
				% Blaue Labels
				\node[blue] at (-0.3,1.3) {$x_1$};
				\node[blue] at (-0.3,2.3) {$x_2$};
				\node[blue] at (-0.3,1.7) {$x_N$};
				
				% Text oben
				\node[red] at (2,3.4) {\small discrete variables};
				
				% Roter Intervalltext
				\draw[red, thick, decorate, decoration={brace, amplitude=5pt, mirror}] (1,-0.4) -- (2,-0.4)
				node[midway, below=4pt, red] {\scriptsize this interval must go to zero};
			\end{scope}
			
			
			% --- Pfeil zwischen den Diagrammen ---
			\draw[->, very thick] (4.7,1.5) -- (5.7,1.5);
			
			
			% --- Rechtes Diagramm: kontinuierliche Variable ---
			\begin{scope}[xshift=6.8cm]
				% Achsen
				\draw[->] (0,0) -- (3.8,0) node[right] {$s$};
				\draw[->] (0,-0.3) -- (0,3) node[left] {$x$};
				
				% Blaue Kurve (x(s))
				\draw[blue, thick, smooth]
				plot coordinates {(0.8,1.8) (1.8,2.6) (2.9,1.2) (3.3,1.4)};
				
				% Vertikale Linien bei s1 und s2
				\draw[dashed, blue] (0.8,0) -- (0.8,1.8);
				\draw[dashed, blue] (3.3,0) -- (3.3,1.4);
				
				% Horizontale Hilfslinien
				\draw[dashed, blue] (0,1.8) -- (0.8,1.8);
				\draw[dashed, blue] (0,1.4) -- (3.3,1.4);
				
				% Beschriftungen
				\node[below] at (0.8,0) {$s_1$};
				\node[below] at (3.3,0) {$s_2$};
				\node[blue] at (-0.5,1.9) {$x(s_1)$};
				\node[blue] at (-0.5,1.3) {$x(s_2)$};
				\node[blue] at (2.6,2.8) {$x(s)$};
				
				% Text oben
				\node[red] at (2,3.4) {\small continuous variables};
			\end{scope}
			
		\end{tikzpicture}
		\end{center}
		
		\[
		\min_{x(\cdot)} \; \int_{s_1}^{s_2} f\big(s, x(s), \dot{x}(s)\big) \, ds
		\]
		\[
		\text{s.t.} \quad
		\begin{aligned}
			&x(s) \in \mathcal{X} \subseteq \mathbb{R}^n, \quad s \in [s_1, s_2],\\
			&x(s_1) = x_1,
		\end{aligned}
		\quad
		\left\{
		\begin{array}{l}
			\text{prototypical CV problem}\\[4pt]
			\bullet\; \text{no ODE yet}\\[4pt]
			\bullet\; \text{opt. variable lives in a function space}
		\end{array}
		\right.
		\]
	\end{itemize}
	
	\subsection{Introduction to CV (Calculus of Variations)}
	
	Function CLASSES \& NORMS
	
	\[
		(\underset{\text{vector space}}{V},\underset{\text{norm}}{\norm{\cdot}})\quad\text{normed vector space}
	\]
	
	$V$ is the set of vector functions
	\[
		x(s),\quad s\in[s_1,s_2]\text{ taking values in }\mathbb{R}^n,\quad[s_1,s_2]\subseteq\mathbb{R}
	\]
	Two classes:
	\begin{itemize}
		\item $V = \mathcal{C}^1([s_1, s_2], \mathbb{R}^n)$: continuously differentiable functions $x: [s_1, s_2] \to \mathbb{R}^n$
		\item $\hat{V} = \hat{\mathcal{C}}^1([s_1, s_2], \mathbb{R}^n)$: piecewise continuously differentiable functions
		
		\hspace*{3.6cm} $x: [s_1, s_2] \to \mathbb{R}^n$.
	\end{itemize}
	
	\begin{definitionbox}[Piecewise continuously differentiable functions]
			$x: [s_1, s_2] \to \mathbb{R}^n$ is piecewise continuously differentiable (PCD) if
		\begin{itemize}
			\item $x \in \mathcal{C}\text{ on }[s_1, s_2]$,
			\item $\exists$ a finite partition $\{c_k\}_{k=0}^{N+1}$ with 
			\[
			s_1 = c_0 < c_1 < \dots < c_{N+1} = s_2,
			\]
			such that $x: [c_k, c_{k+1}] \to \mathbb{R}^n$ is $\mathcal{C}^1$.
			
			That is $x \in \mathcal{C}^1([c_k, c_{k+1}], \mathbb{R}^n), \quad \forall k = 0, 1, \dots, N$.
		\end{itemize}
	\end{definitionbox}
	
	\textbf{Example:}\nopagebreak\noindent
	\begin{center}
		\begin{tikzpicture}[scale=1.2, >=Latex]
			
			% Achsen
			\draw[->] (0,0) -- (6,0) node[right] {$s$};
			\draw[->] (0.5,-0.5) -- (0.5,2.8) node[left] {$x$};
			
			% Achsenbeschriftungen
			\node[below] at (1,0) {$s_1$};
			\node[below] at (2.3,0) {$c_1$};
			\node[below] at (3.8,0) {$c_2$};
			\node[below] at (5.2,0) {$s_2$};
			
			% Blaue Kurve: stÃ¼ckweise C^1 mit sichtbaren Knicken
			\draw[blue, thick, smooth, tension=0.9]
			plot coordinates {(1,0.0) (1.4,1.0) (1.9,0.8) (2.3,1.3)};
			% Knick an c1
			\draw[blue, thick, smooth, tension=0.8]
			plot coordinates {(2.3,1.3) (2.8,0.7) (3.3,1.0) (3.8,0.6)};
			% Knick an c2
			\draw[blue, thick, smooth, tension=0.8]
			plot coordinates {(3.8,0.6) (4.3,1.0) (4.7,1.4) (5.2,2.0)};
			
			% Eckenpunkte (rot)
			\filldraw[red] (2.3,1.3) circle (2pt);
			\filldraw[red] (3.8,0.6) circle (2pt);
			
			% Gelbe Markierungen um die Ecken
			\foreach \x/\y in {2.3/1.3, 3.8/0.6} {
				\draw[fill=yellow, opacity=0.3, draw=none] (\x,\y) circle (6pt);
			}
			
			% Rotes Intervall [c2, s2]
			\draw[line width=4pt, red!40, opacity=0.5] (3.8,0) -- (5.2,0);
			
			% Textbeschriftungen
			\node[blue] at (2.7,2.2) {$x(s) \in \hat{\mathcal{C}}^1$};
			
			\node[purple, align=left] (text1) at (6.4,1.0)
			{Inside the interval\\ it is a $\mathcal{C}^1$ function};
			\draw[purple, ->, thick] (text1.west) -- (4.5,0.1);
			
			\node[purple, align=center] (text2) at (3.0,-1.2)
			{2 corner points in this case};
			\draw[purple, ->, thick] (text2.north) ++ (-0.5,0.0) -- (2.3,-0.5);
			\draw[purple, ->, thick] (text2.north) ++ (0.5,0.0) -- (3.8,-0.5);
			
		\end{tikzpicture}
	\end{center}
	
\end{document}
